{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading aisles...\n",
      "loading department...\n",
      "loading products...\n",
      "loading prior orders...\n",
      "loading train orders...\n",
      "loading orders...\n",
      "loading None products in prior\n",
      "loading None products in train\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from collections import Counter\n",
    "%matplotlib inline\n",
    "\n",
    "print('loading aisles...')\n",
    "aisles = pd.read_csv('aisles_.csv', dtype={\n",
    "        'aisle_id': np.uint16,\n",
    "        'aisle': 'category'})\n",
    "\n",
    "print('loading department...')\n",
    "department = pd.read_csv('departments_.csv', dtype={\n",
    "            'department_id': np.uint8,\n",
    "            'department': 'category'})\n",
    "\n",
    "print('loading products...')\n",
    "products = pd.read_csv('products_.csv', dtype={\n",
    "        'product_id': np.uint16,\n",
    "        'order_id': np.uint32,\n",
    "        'aisle_id': np.uint8,\n",
    "        'department_id': np.uint8})\n",
    "\n",
    "print('loading prior orders...')\n",
    "prior = pd.read_csv('order_products__prior_.csv', dtype={\n",
    "        'order_id': np.uint32,\n",
    "        'product_id': np.uint16,\n",
    "        'add_to_cart_order': np.uint16,\n",
    "        'reordered': np.uint16})\n",
    "\n",
    "print('loading train orders...')\n",
    "train = pd.read_csv('order_products__train_.csv', dtype={\n",
    "        'order_id': np.uint32,\n",
    "        'product_id': np.uint16,\n",
    "        'add_to_cart_order': np.uint16,\n",
    "        'reordered': np.uint8})\n",
    "\n",
    "print('loading orders...')\n",
    "order = pd.read_csv('orders_.csv' , dtype={\n",
    "        'order_id': np.uint32,\n",
    "        'user_id': np.uint32,\n",
    "        'eval_set': 'category',\n",
    "        'order_number': np.uint16,\n",
    "        'order_dow': np.uint16,\n",
    "        'order_hour_of_day': np.uint16,\n",
    "        'days_since_prior_order': np.float32})\n",
    "\n",
    "print('loading None products in prior')\n",
    "prior_None = pd.read_csv('None_order_prior_.csv', dtype={\n",
    "            'order_id': np.uint32,\n",
    "            'product_id': np.uint16,\n",
    "            'add_to_cart_order': np.uint16,\n",
    "            'reordered': np.uint16})\n",
    "\n",
    "print('loading None products in train')\n",
    "train_None = pd.read_csv('None_order_train_.csv', dtype={\n",
    "            'order_id': np.uint32,\n",
    "            'product_id': np.uint16,\n",
    "            'add_to_cart_order': np.uint16,\n",
    "            'reordered': np.uint16})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior = pd.concat([prior, prior_None], ignore_index = True)\n",
    "train = pd.concat([train, train_None], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# order.set_index('order_id', inplace=True, drop=False)\n",
    "# reorder_info = pd.DataFrame()\n",
    "# reorder_info['order_id'] = prior.groupby(prior.order_id)['order_id'].apply(lambda x: x.iloc[0])\n",
    "# reorder_info['user_id'] = reorder_info.order_id.map(order.user_id)\n",
    "# reorder_info['nb_reorder'] = prior.groupby(prior.order_id)['reordered'].sum().astype(np.uint8)\n",
    "# reorder_info['last_add_to_cart_order'] = prior.groupby(prior.order_id)['add_to_cart_order'].max()\n",
    "# reorder_info['order_number'] = reorder_info.order_id.map(order.order_number)\n",
    "# none_order_id = np.array(reorder_info.order_id[reorder_info.nb_reorder == 0])\n",
    "# d = dict()\n",
    "# d['order_id'] = []\n",
    "# d['product_id'] = []\n",
    "# d['add_to_cart_order'] = []\n",
    "# d['reordered'] = []\n",
    "# count=0\n",
    "# for k in none_order_id:\n",
    "#     count+=1\n",
    "#     if not count % 10000:\n",
    "#         print(\"%d orders\" % count)\n",
    "#     order_number = np.array(order.order_number[order.order_id == k])[0]\n",
    "#     if order_number > 1: # 第一个order不要\n",
    "#         d['order_id'].append(k)\n",
    "#         d['product_id'].append(49689)\n",
    "#         d['add_to_cart_order'].append(np.array(reorder_info.last_add_to_cart_order[reorder_info.order_id==k])[0] + 1)\n",
    "#         if order_number == 2: # 第二个order不算reorder\n",
    "#             d['reordered'].append(0)\n",
    "#         else:\n",
    "#             user_id = np.array(reorder_info.user_id[reorder_info.order_id==k])[0]\n",
    "#             user_orders = order[order.user_id==user_id]\n",
    "#             last_order_id = np.array(user_orders.order_id[user_orders.order_number == (order_number - 1)])[0]\n",
    "#             if np.array(reorder_info.nb_reorder[reorder_info.order_id==last_order_id])[0] == 0:\n",
    "#                 d['reordered'].append(1)\n",
    "#             else:\n",
    "#                 d['reordered'].append(0)\n",
    "# tmp = pd.DataFrame()\n",
    "# tmp['order_id'] = d['order_id']\n",
    "# tmp['product_id'] = d['product_id']\n",
    "# tmp['add_to_cart_order'] = d['add_to_cart_order']\n",
    "# tmp['reordered'] = d['reordered']\n",
    "# tmp.to_csv('None_order_prior.csv', index=False)\n",
    "# reorder_info_train = pd.DataFrame()\n",
    "# reorder_info_train['order_id'] = train.groupby(train.order_id)['order_id'].apply(lambda x: x.iloc[0])\n",
    "# reorder_info_train['user_id'] = reorder_info_train.order_id.map(order.user_id)\n",
    "# reorder_info_train['nb_reorder'] = train.groupby(train.order_id)['reordered'].sum().astype(np.uint8)\n",
    "# reorder_info_train['last_add_to_cart_order'] = train.groupby(train.order_id)['add_to_cart_order'].max()\n",
    "# reorder_info_train['order_number'] = reorder_info_train.order_id.map(order.order_number)\n",
    "# none_order_id_train = np.array(reorder_info_train.order_id[reorder_info_train.nb_reorder == 0])\n",
    "\n",
    "# d = dict()\n",
    "# d['order_id'] = []\n",
    "# d['product_id'] = []\n",
    "# d['add_to_cart_order'] = []\n",
    "# d['reordered'] = []\n",
    "# count=0\n",
    "# for k in none_order_id_train:\n",
    "#     count+=1\n",
    "#     if not count % 10000:\n",
    "#         print(\"%d orders\" % count)\n",
    "#     order_number = np.array(order.order_number[order.order_id == k])[0]\n",
    "#     d['order_id'].append(k)\n",
    "#     d['product_id'].append(49689)\n",
    "#     d['add_to_cart_order'].append(np.array(reorder_info_train.last_add_to_cart_order[reorder_info_train.order_id==k])[0] + 1)\n",
    "#     user_id = np.array(reorder_info_train.user_id[reorder_info_train.order_id==k])[0]\n",
    "#     user_orders = order[order.user_id==user_id]\n",
    "# #     display(user_orders)\n",
    "#     last_order_id = np.array(user_orders.order_id[user_orders.order_number == (order_number - 1)])[0]\n",
    "#     if np.array(reorder_info.nb_reorder[reorder_info.order_id==last_order_id])[0] == 0:\n",
    "#         d['reordered'].append(1)\n",
    "#     else:\n",
    "#         d['reordered'].append(0)\n",
    "# tmp = pd.DataFrame()\n",
    "# tmp['order_id'] = d['order_id']\n",
    "# tmp['product_id'] = d['product_id']\n",
    "# tmp['add_to_cart_order'] = d['add_to_cart_order']\n",
    "# tmp['reordered'] = d['reordered']\n",
    "# tmp.to_csv('None_order_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# best sellers\n",
    "best_seller_id = [24852, 13176, 21137, 21903, 47626, 47766, 47209, 16797, \\\n",
    "                 26209, 27966]\n",
    "most_often_reordered = [1729, 20940, 12193, 21038, 31764, 24852, 117, \\\n",
    "                       39180, 12384, 24024]\n",
    "# organic VS non-organic\n",
    "True_organic = np.array(['Organic' in name for name in products.product_name])\n",
    "organic_product_id = np.array(products.product_id[True_organic])\n",
    "non_organic_product_id = np.array(products.product_id[~True_organic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_orders = order[order.eval_set == 'train']\n",
    "test_orders = order[order.eval_set == 'test']\n",
    "prior_orders = order[order.eval_set == 'prior']\n",
    "\n",
    "train.set_index(['order_id', 'product_id'], inplace=True, drop=False)\n",
    "\n",
    "order.set_index('order_id', inplace=True, drop=False)\n",
    "prior = prior.join(order, on='order_id', rsuffix='_')\n",
    "prior.drop('order_id_', inplace=True, axis=1)\n",
    "prior.set_index('order_id', inplace=True, drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct products information...\n"
     ]
    }
   ],
   "source": [
    "print('Construct products information...')\n",
    "prods = pd.DataFrame()\n",
    "prods['total_nb'] = prior.groupby(prior.product_id).size().astype(np.uint32)\n",
    "prods['nb_reorder'] = prior.groupby(prior.product_id)['reordered'].sum().astype(np.uint32)\n",
    "prods['reorder_rate'] = prods.nb_reorder / prods.total_nb.astype(np.float32)\n",
    "prods['nb_buyers'] = prior.groupby(prior.product_id)['user_id'].apply(lambda x: len(set(x))).astype(np.uint16) # unique buyers\n",
    "prods['avg_add_to_cart_order'] = prior.groupby(prior.product_id)['add_to_cart_order'].mean().astype(np.uint8)\n",
    "prods['min_add_to_cart_order'] = prior.groupby(prior.product_id)['add_to_cart_order'].min().astype(np.uint8)\n",
    "prods['max_add_to_cart_order'] = prior.groupby(prior.product_id)['add_to_cart_order'].max().astype(np.uint8)\n",
    "prods['nb_orders'] = prior.groupby(prior.product_id).size().astype(np.uint16)\n",
    "products = products.join(prods, on='product_id')\n",
    "products.set_index('product_id', drop=False, inplace=True)\n",
    "products['is_organic'] = np.uint8(True_organic)\n",
    "del prods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order related information...\n"
     ]
    }
   ],
   "source": [
    "print('Order related information...')\n",
    "ords_pri = pd.DataFrame()\n",
    "ords_pri['order_id'] = prior.groupby(prior.order_id)['order_id'].apply(lambda x: x.iloc[0])\n",
    "ords_pri['user_id'] = prior.groupby(prior.order_id)['user_id'].apply(lambda x: x.iloc[0])\n",
    "ords_pri['unique_product_id'] = prior.groupby(prior.order_id)['product_id'].apply(set)\n",
    "ords_pri['all_product_id'] = prior.groupby(prior.order_id)['product_id'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prior['is_organic'] = prior.product_id.map(products.is_organic)\n",
    "ords_pri['nb_organic_items'] = prior.groupby(prior.order_id)['is_organic'].sum().astype(np.uint16)\n",
    "# ords_pri['nb_reorder_organic_items'] = prior.groupby(prior.order_id).apply(lambda subframe: sum(subframe['is_organic'][subframe.reordered == 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ords_pri.set_index('order_id', drop=False, inplace=True)\n",
    "ords_pri['nb_items'] = prior.groupby(prior.order_id)['product_id'].size().astype(np.uint8)\n",
    "ords_pri['first_item_id'] = prior.groupby(prior.order_id)['product_id'].apply(lambda x: x.iloc[0])\n",
    "ords_pri['first_item_reorder'] = prior.groupby(prior.order_id)['reordered'].apply(lambda x: x.iloc[0])\n",
    "ords_pri['nb_reorder'] = prior.groupby(prior.order_id)['reordered'].sum()\n",
    "ords_pri['reorder_ratio'] = (ords_pri['nb_reorder'] / ords_pri['nb_items']).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min and max basket size for each user...\n",
      "10000 orders...\n",
      "20000 orders...\n",
      "30000 orders...\n",
      "40000 orders...\n",
      "50000 orders...\n",
      "60000 orders...\n",
      "70000 orders...\n",
      "80000 orders...\n",
      "90000 orders...\n",
      "100000 orders...\n",
      "110000 orders...\n",
      "120000 orders...\n",
      "130000 orders...\n",
      "140000 orders...\n",
      "150000 orders...\n",
      "160000 orders...\n",
      "170000 orders...\n",
      "180000 orders...\n",
      "190000 orders...\n",
      "200000 orders...\n"
     ]
    }
   ],
   "source": [
    "print(\"min and max basket size for each user...\")\n",
    "min_basket_list = []\n",
    "max_basket_list = []\n",
    "avg_basket_list = []\n",
    "nb_reorder_items_list = []\n",
    "avg_reorder_per_basket_list = []\n",
    "reorder_order_vs_order_ratio_list = []\n",
    "nb_all_items_list = []\n",
    "nb_reorder_items_vs_nb_all_items_ratio_list = []\n",
    "min_reorder_items_list = []\n",
    "max_reorder_items_list = []\n",
    "unique_items_set = []\n",
    "all_product_list = []\n",
    "reorder_product_list = []\n",
    "reorder_unique_product_set = []\n",
    "nb_unique_items_list = []\n",
    "nb_organic_items = []\n",
    "organic_ratio_list = []\n",
    "avg_organic_basket = []\n",
    "# nb_organic_reordered = []\n",
    "# organic_reorder_ratio = []\n",
    "# organic_reorder_ratio_VS_all = []\n",
    "user_order_list = prior_orders.groupby('user_id')['order_id'].apply(list)\n",
    "for ord_i, order_list in enumerate(user_order_list):\n",
    "    if not (ord_i + 1) % 10000:\n",
    "        print(\"%d orders...\" % (ord_i+1))\n",
    "    min_basket=999\n",
    "    max_basket=0\n",
    "    min_reorder_basket = 999\n",
    "    max_reorder_basket = 0\n",
    "    min_reorder_items = 999\n",
    "    max_reorder_items = 0\n",
    "    nb_reorder = 0\n",
    "    nb_reorder_orders = 0\n",
    "    nb_all_items = 0\n",
    "    nb_order = len(order_list)\n",
    "    all_product_list_tmp = []\n",
    "    nb_organic_items_tmp = 0\n",
    "    nb_organic_reordered_tmp = 0\n",
    "    unique_items_tmp_set = set()\n",
    "    for order_id in order_list:\n",
    "        nb_item_s = ords_pri.loc[order_id, 'nb_items']\n",
    "        nb_all_items += nb_item_s\n",
    "        nb_reorder_s = ords_pri.loc[order_id, 'nb_reorder']\n",
    "        nb_reorder += nb_reorder_s\n",
    "        nb_reorder_orders += (ords_pri.loc[order_id, 'nb_reorder'] > 0).astype(np.uint8)\n",
    "        min_basket = min(min_basket, nb_item_s)\n",
    "        max_basket = max(max_basket, nb_item_s)\n",
    "        min_reorder_items = min(min_reorder_items, nb_reorder_s)\n",
    "        max_reorder_items = max(max_reorder_items, nb_reorder_s)\n",
    "        unique_items_tmp_set |= ords_pri.loc[order_id, 'unique_product_id']\n",
    "        all_product_list_tmp += ords_pri.loc[order_id, 'all_product_id']\n",
    "        nb_organic_items_tmp += ords_pri.loc[order_id, 'nb_organic_items']\n",
    "#         nb_organic_reordered_tmp += ords_pri.loc[order_id, 'nb_organic_reorder_items']\n",
    "    unique_items_set.append(list(unique_items_tmp_set))\n",
    "    all_product_list.append(all_product_list_tmp)\n",
    "    nb_unique_items_list.append(len(unique_items_set))\n",
    "    avg_basket_list.append((nb_all_items / nb_order).astype(np.float32))\n",
    "    min_basket_list.append(min_basket)\n",
    "    max_basket_list.append(max_basket)\n",
    "    nb_reorder_items_list.append(nb_reorder)\n",
    "    avg_reorder_per_basket_list.append((nb_reorder / nb_order).astype(np.float32))\n",
    "    reorder_order_vs_order_ratio_list.append((nb_reorder_orders / nb_order).astype(np.float32)) \n",
    "    nb_all_items_list.append(nb_all_items)\n",
    "    nb_reorder_items_vs_nb_all_items_ratio_list.append((nb_reorder / nb_all_items).astype(np.float32))\n",
    "    min_reorder_items_list.append(min_reorder_items)\n",
    "    max_reorder_items_list.append(max_reorder_items)\n",
    "    nb_organic_items.append(nb_organic_items_tmp)\n",
    "    organic_ratio_list.append(np.float32(nb_organic_items_tmp / nb_all_items))\n",
    "    avg_organic_basket.append(np.float32(nb_organic_items_tmp / nb_order))\n",
    "#     nb_organic_reordered.append(nb_organic_items_tmp)\n",
    "#     organic_reorder_ratio.append(float32(nb_organic_reordered_tmp / (nb_organic_items_tmp + 1e-6)))\n",
    "#     organic_reorder_ratio_VS_all.append(float32(nb_organic_reordered_tmp / nb_all_items))\n",
    "#     print(\"order id: %d, min_basket: %d, max_basket: %d\" %(order_id, min_basket, max_basket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userXdepartment and aisle features\n",
      "10000 orders...\n",
      "20000 orders...\n",
      "30000 orders...\n",
      "40000 orders...\n",
      "50000 orders...\n",
      "60000 orders...\n",
      "70000 orders...\n",
      "80000 orders...\n",
      "90000 orders...\n",
      "100000 orders...\n",
      "110000 orders...\n",
      "120000 orders...\n",
      "130000 orders...\n",
      "140000 orders...\n",
      "150000 orders...\n",
      "160000 orders...\n",
      "170000 orders...\n",
      "180000 orders...\n",
      "190000 orders...\n",
      "200000 orders...\n"
     ]
    }
   ],
   "source": [
    "print(\"userXdepartment and aisle features\")\n",
    "user_id_list = list(prior.groupby('user_id')['user_id'].apply(lambda x: x.iloc[0]))\n",
    "nb_unique_departments = [] #\n",
    "nb_unique_aisles = [] #\n",
    "favorite_department_id = [] #\n",
    "favorite_aisle_id = [] #\n",
    "nb_unique_reorder_departments = [] #\n",
    "nb_unique_reorder_aisles = [] #\n",
    "favorite_reorder_department_id = [] #\n",
    "favorite_reorder_aisle_id = [] #\n",
    "depart_reorder_ratio = [] # unique_nb_reorder_depart / unique_nb_depart\n",
    "aisle_reorder_ratio = [] #\n",
    "count=0\n",
    "for i, pid_list in enumerate(all_product_list):\n",
    "    count+=1\n",
    "    if not count % 10000:\n",
    "        print(\"%d orders...\" % count)\n",
    "    pri_uid = prior[prior.user_id == user_id_list[i]]\n",
    "    reorder_product_list = list(pri_uid.product_id[pri_uid.reordered == 1])\n",
    "    reorder_unique_product_list = set(reorder_product_list)\n",
    "    reorder_departments_tmp = []\n",
    "    reorder_aisles_tmp = []\n",
    "    for re_pid in reorder_unique_product_list:\n",
    "        reorder_departments_tmp.append(products.loc[re_pid, 'department_id'])\n",
    "        reorder_aisles_tmp.append(products.loc[re_pid, 'aisle_id'])\n",
    "    nb_unique_reorder_departments.append(len(set(reorder_departments_tmp)))\n",
    "    nb_unique_reorder_aisles.append(len(set(reorder_aisles_tmp)))\n",
    "    counter_res = Counter(nb_unique_reorder_departments).most_common(1)\n",
    "    favorite_reorder_department_id.append(counter_res[0][0])\n",
    "    counter_res = Counter(nb_unique_reorder_aisles).most_common(1)\n",
    "    favorite_reorder_aisle_id.append(counter_res[0][0])\n",
    "    \n",
    "    department_list_tmp = []\n",
    "    aisle_list_tmp = []\n",
    "    for pid in pid_list:\n",
    "        department_list_tmp.append(products.loc[pid, 'department_id'])\n",
    "        aisle_list_tmp.append(products.loc[pid, 'aisle_id'])\n",
    "    nb_unique_departments.append(len(set(department_list_tmp)))\n",
    "    nb_unique_aisles.append(len(set(aisle_list_tmp)))\n",
    "    counter_res = Counter(department_list_tmp).most_common(2)\n",
    "    favorite_department_id.append(counter_res[0][0])\n",
    "    counter_res = Counter(aisle_list_tmp).most_common(2)\n",
    "    favorite_aisle_id.append(counter_res[0][0])\n",
    "    \n",
    "    depart_reorder_ratio.append(len(set(reorder_departments_tmp)) / len(set(department_list_tmp)))\n",
    "    aisle_reorder_ratio.append(len(set(reorder_aisles_tmp)) / len(set(aisle_list_tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_orders.set_index('user_id', drop=False, inplace=True)\n",
    "prior_orders.set_index('user_id', drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User related information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:30: FutureWarning: 'user_id' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "/opt/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:31: FutureWarning: 'user_id' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "/opt/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:32: FutureWarning: 'user_id' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "/opt/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:33: FutureWarning: 'user_id' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "/opt/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:34: FutureWarning: 'user_id' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users best 10 sellers\n",
      "users best 10 reorder\n"
     ]
    }
   ],
   "source": [
    "print('User related information')\n",
    "users = pd.DataFrame()\n",
    "users['user_id'] = user_id_list\n",
    "users.set_index('user_id', inplace=True, drop=False)\n",
    "users['all_product'] = all_product_list\n",
    "users['unique_product'] = nb_unique_items_list\n",
    "\n",
    "users['nb_order'] = prior_orders.groupby(users.user_id).size().astype(np.uint16)\n",
    "users['orders'] = prior_orders.groupby(users.user_id)['order_id'].apply(list) \n",
    "users['min_basket'] = min_basket_list\n",
    "users['max_basket'] = max_basket_list\n",
    "users['nb_reorder_items'] = nb_reorder_items_list\n",
    "users['avg_reorder_per_basket'] = avg_reorder_per_basket_list\n",
    "users['reorder_order_vs_order_ratio'] = reorder_order_vs_order_ratio_list\n",
    "users['nb_all_items'] = nb_all_items_list\n",
    "users['nb_reorder_items_vs_nb_all_items_ratio'] = nb_reorder_items_vs_nb_all_items_ratio_list\n",
    "users['min_reorder_items'] = min_reorder_items_list\n",
    "users['max_reorder_items'] = max_reorder_items_list\n",
    "users['nb_unique_items'] = nb_unique_items_list\n",
    "users['avg_basket'] = avg_basket_list\n",
    "users['all_products'] = all_product_list\n",
    "users['unique_products'] = unique_items_set\n",
    "users['nb_organic_items'] = nb_organic_items\n",
    "users['organic_ratio'] = organic_ratio_list\n",
    "users['avg_organic_basket'] = avg_organic_basket\n",
    "# users['nb_organic_reordered'] = nb_organic_reordered\n",
    "# users['organic_reorder_ratio'] = organic_reorder_ratio\n",
    "# users['organic_reorder_ratio_VS_all'] = organic_reorder_ratio_VS_all\n",
    "\n",
    "users['avg_days_between_order'] = prior_orders.groupby('user_id')['days_since_prior_order'].mean().astype(np.float32)\n",
    "users['sum_days_between_order'] = prior_orders.groupby('user_id')['days_since_prior_order'].sum().astype(np.uint16)\n",
    "users['avg_hour_of_day'] = prior_orders.groupby('user_id')['order_hour_of_day'].mean().astype(np.float32)\n",
    "users['min_days_of_week'] = prior_orders.groupby('user_id')['order_dow'].apply(min).astype(np.uint8)\n",
    "users['max_days_of_week'] = prior_orders.groupby('user_id')['order_dow'].apply(max).astype(np.uint8)\n",
    "print('users best 10 sellers')\n",
    "users['nb_1st_seller'] = users.all_products.apply(lambda X: sum([best_seller_id[0]==x for x in X]))\n",
    "users['nb_2nd_seller'] = users.all_products.apply(lambda X: sum([best_seller_id[1]==x for x in X]))\n",
    "users['nb_3rd_seller'] = users.all_products.apply(lambda X: sum([best_seller_id[2]==x for x in X]))\n",
    "users['nb_4th_seller'] = users.all_products.apply(lambda X: sum([best_seller_id[3]==x for x in X]))\n",
    "users['nb_5th_seller'] = users.all_products.apply(lambda X: sum([best_seller_id[4]==x for x in X]))\n",
    "users['nb_6th_seller'] = users.all_products.apply(lambda X: sum([best_seller_id[5]==x for x in X]))\n",
    "users['nb_7th_seller'] = users.all_products.apply(lambda X: sum([best_seller_id[6]==x for x in X]))\n",
    "users['nb_8th_seller'] = users.all_products.apply(lambda X: sum([best_seller_id[7]==x for x in X]))\n",
    "users['nb_9th_seller'] = users.all_products.apply(lambda X: sum([best_seller_id[8]==x for x in X]))\n",
    "users['nb_10th_seller'] = users.all_products.apply(lambda X: sum([best_seller_id[9]==x for x in X]))\n",
    "print('users best 10 reorder')\n",
    "users['nb_1st_reorder'] = users.all_products.apply(lambda X: sum([most_often_reordered[0] == x for x in X]))\n",
    "users['nb_2nd_reorder'] = users.all_products.apply(lambda X: sum([most_often_reordered[1] == x for x in X]))\n",
    "users['nb_3rd_reorder'] = users.all_products.apply(lambda X: sum([most_often_reordered[2] == x for x in X]))\n",
    "users['nb_4th_reorder'] = users.all_products.apply(lambda X: sum([most_often_reordered[3] == x for x in X]))\n",
    "users['nb_5th_reorder'] = users.all_products.apply(lambda X: sum([most_often_reordered[4] == x for x in X]))\n",
    "users['nb_6th_reorder'] = users.all_products.apply(lambda X: sum([most_often_reordered[5] == x for x in X]))\n",
    "users['nb_7th_reorder'] = users.all_products.apply(lambda X: sum([most_often_reordered[6] == x for x in X]))\n",
    "users['nb_8th_reorder'] = users.all_products.apply(lambda X: sum([most_often_reordered[7] == x for x in X]))\n",
    "users['nb_9th_reorder'] = users.all_products.apply(lambda X: sum([most_often_reordered[8] == x for x in X]))\n",
    "users['nb_10th_reorder'] = users.all_products.apply(lambda X: sum([most_often_reordered[9] for x in X]))\n",
    "\n",
    "users['last_order_id'] = prior_orders.groupby(prior_orders.user_id)['order_id'].apply(lambda x: x.iloc[-1])\n",
    "users['lo_nb_products'] = users.last_order_id.map(ords_pri.nb_items)\n",
    "users['lo_first_item_id'] = users.last_order_id.map(ords_pri.first_item_id)\n",
    "users['lo_first_item_reorder'] = users.last_order_id.map(ords_pri.first_item_reorder)\n",
    "users['lo_nb_reorder'] = users.last_order_id.map(ords_pri.nb_reorder)\n",
    "users['lo_reorder_ratio'] = users.last_order_id.map(ords_pri.reorder_ratio)\n",
    "\n",
    "users['last_2_order_id'] = prior_orders.groupby(prior_orders.user_id)['order_id'].apply(lambda x: x.iloc[-2])\n",
    "users['lo2_nb_products'] = users.last_2_order_id.map(ords_pri.nb_items)\n",
    "users['lo2_first_item_id'] = users.last_2_order_id.map(ords_pri.first_item_id)\n",
    "users['lo2_first_item_reorder'] = users.last_2_order_id.map(ords_pri.first_item_reorder)\n",
    "users['lo2_nb_reorder'] = users.last_2_order_id.map(ords_pri.nb_reorder)\n",
    "users['lo2_reorder_ratio'] = users.last_2_order_id.map(ords_pri.reorder_ratio)\n",
    "\n",
    "users['last_3_order_id'] = prior_orders.groupby(prior_orders.user_id)['order_id'].apply(lambda x: x.iloc[-3])\n",
    "users['lo3_nb_products'] = users.last_3_order_id.map(ords_pri.nb_items)\n",
    "users['lo3_first_item_id'] = users.last_3_order_id.map(ords_pri.first_item_id)\n",
    "users['lo3_first_item_reorder'] = users.last_3_order_id.map(ords_pri.first_item_reorder)\n",
    "users['lo3_nb_reorder'] = users.last_3_order_id.map(ords_pri.nb_reorder)\n",
    "users['lo3_reorder_ratio'] = users.last_3_order_id.map(ords_pri.reorder_ratio)\n",
    "\n",
    "users['nb_unique_departments'] = nb_unique_departments\n",
    "users['nb_unique_aisles'] = nb_unique_aisles\n",
    "users['favorite_department_id'] = favorite_department_id\n",
    "users['favorite_aisle_id'] = favorite_aisle_id\n",
    "users['nb_unique_reorder_departments'] = nb_unique_reorder_departments\n",
    "users['nb_unique_reorder_aisles'] = nb_unique_reorder_aisles\n",
    "users['favorite_reorder_department_id'] = favorite_reorder_department_id\n",
    "users['favorite_reorder_aisle_id'] = favorite_reorder_aisle_id\n",
    "users['depart_reorder_ratio'] = depart_reorder_ratio\n",
    "users['aisle_reorder_ratio'] = aisle_reorder_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserXproduct_id information...\n"
     ]
    }
   ],
   "source": [
    "print('UserXproduct_id information...')\n",
    "prior['user_product_index'] = (prior.user_id.astype(np.uint64) * 100000\\\n",
    "                               + prior.product_id).astype(np.uint64)\n",
    "d = dict()\n",
    "for row in prior.itertuples():\n",
    "    k = row.user_product_index\n",
    "    if k not in d:\n",
    "        d[k] = (1, \\\n",
    "                row.add_to_cart_order, \\\n",
    "                row.reordered, \\\n",
    "                (row.order_number, row.order_id),\\\n",
    "                row.order_dow, \\\n",
    "                row.order_hour_of_day, \\\n",
    "                row.add_to_cart_order, \\\n",
    "                row.add_to_cart_order, \\\n",
    "                (row.order_number == 1).astype(np.uint8), \\\n",
    "                (row.order_number == 2).astype(np.uint8), \\\n",
    "                (row.order_number, row.order_id))\n",
    "    else:\n",
    "        d[k] = (d[k][0]+1, d[k][1]+row.add_to_cart_order, \\\n",
    "                d[k][2]+row.reordered, \\\n",
    "                # find last order with that product\n",
    "                max(d[k][3], (row.order_number, row.order_id)), \\\n",
    "                d[k][4]+row.order_dow, \\\n",
    "                d[k][5]+row.order_hour_of_day, \\\n",
    "                min(d[k][6], row.add_to_cart_order), \\\n",
    "                max(d[k][7], row.add_to_cart_order), \\\n",
    "                d[k][8]+(row.order_number == 1).astype(np.uint8), \\\n",
    "                d[k][9]+(row.order_number == 2).astype(np.uint8), \\\n",
    "                min(d[k][10], (row.order_number, row.order_id)))\n",
    "UserProduct = pd.DataFrame.from_dict(d, orient='index')\n",
    "del d\n",
    "UserProduct.columns = ['nb_orders', 'sum_add_to_cart_order', 'nb_reordered', \\\n",
    "                      'last_order_id', 'sum_order_dow', 'sum_order_hour_of_day', \\\n",
    "                      'min_add_to_cart_order', 'max_add_to_cart_order', \\\n",
    "                       'buy_first_time_total_nb', 'buy_second_time_total_nb', \\\n",
    "                       'first_order_id']\n",
    "UserProduct['nb_orders'] = UserProduct.nb_orders.astype(np.uint16) \n",
    "UserProduct['sum_add_to_cart_order'] = UserProduct.sum_add_to_cart_order.astype(np.uint16)\n",
    "UserProduct['nb_reordered'] = UserProduct.nb_reordered.astype(np.uint16)\n",
    "UserProduct['last_order_id'] = UserProduct.last_order_id.map(lambda x: x[1]).astype(np.uint32)\n",
    "UserProduct['last_order_number'] = UserProduct.last_order_id.map(order.order_number)\n",
    "UserProduct['first_order_id'] = UserProduct.first_order_id.map(lambda x: x[1]).astype(np.uint32)\n",
    "UserProduct['first_order_number'] = UserProduct.first_order_id.map(order.order_number)\n",
    "UserProduct['sum_order_dow'] = UserProduct.sum_order_dow.astype(np.uint16)\n",
    "UserProduct['sum_order_hour_of_day'] = UserProduct.sum_order_hour_of_day.astype(np.uint32)\n",
    "UserProduct['min_add_to_cart_order'] = UserProduct.min_add_to_cart_order.astype(np.uint8)\n",
    "UserProduct['max_add_to_cart_order'] = UserProduct.max_add_to_cart_order.astype(np.uint8)\n",
    "UserProduct['buy_first_time_total_nb'] = UserProduct.buy_first_time_total_nb\n",
    "UserProduct['buy_second_time_total_nb'] = UserProduct.buy_second_time_total_nb\n",
    "UserProduct['reorder_second_time_VS_first_time'] = (UserProduct.buy_second_time_total_nb - \\\n",
    "                                                    UserProduct.buy_first_time_total_nb).astype(np.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_features(orders, labels_out=False):\n",
    "    print('generate features and labels(optional) from selected orders')\n",
    "    count=0\n",
    "    product_list = []\n",
    "    order_list = []\n",
    "    labels = []\n",
    "    for row in orders.itertuples():\n",
    "        count+=1\n",
    "        order_id = row.order_id\n",
    "        user_id = row.user_id\n",
    "        user_products = users.unique_products[user_id]\n",
    "        product_list += user_products\n",
    "        order_list += [order_id] * len(user_products)\n",
    "        if labels_out:\n",
    "            labels += [(order_id, product) in train.index for product in user_products]            \n",
    "        if count%10000 == 0:\n",
    "            print('order row', count)\n",
    "            \n",
    "    df = pd.DataFrame({'order_id': order_list, 'product_id': product_list}, dtype=np.int32)\n",
    "    labels = np.array(labels, dtype=np.int8)\n",
    "    del order_list\n",
    "    del product_list\n",
    "    \n",
    "    print(\"user related features<prior>\")\n",
    "    df['user_id'] = df.order_id.map(order.user_id)\n",
    "    df['user_total_orders'] = df.user_id.map(users.nb_order)\n",
    "    df['user_avg_days_between_orders'] = df.user_id.map(users.avg_days_between_order)\n",
    "    df['user_sum_days_between_orders'] = df.user_id.map(users.sum_days_between_order)\n",
    "    df['user_min_days_of_week'] = df.user_id.map(users.min_days_of_week)\n",
    "    df['user_max_days_of_week'] = df.user_id.map(users.max_days_of_week)\n",
    "    df['user_avg_hour_of_day'] = df.user_id.map(users.avg_hour_of_day)\n",
    "    df['user_nb_organic_items'] = df.user_id.map(users.nb_organic_items)\n",
    "    \n",
    "    df['user_nb_1st_seller'] = df.user_id.map(users.nb_1st_seller)\n",
    "    df['user_nb_2nd_seller'] = df.user_id.map(users.nb_2nd_seller)\n",
    "    df['user_nb_3rd_seller'] = df.user_id.map(users.nb_3rd_seller)\n",
    "    df['user_nb_4th_seller'] = df.user_id.map(users.nb_4th_seller)\n",
    "    df['user_nb_5th_seller'] = df.user_id.map(users.nb_5th_seller)\n",
    "    df['user_nb_6th_seller'] = df.user_id.map(users.nb_6th_seller)\n",
    "    df['user_nb_7th_seller'] = df.user_id.map(users.nb_7th_seller)\n",
    "    df['user_nb_8th_seller'] = df.user_id.map(users.nb_8th_seller)\n",
    "    df['user_nb_9th_seller'] = df.user_id.map(users.nb_9th_seller)\n",
    "    df['user_nb_10th_seller'] = df.user_id.map(users.nb_10th_seller)\n",
    "    \n",
    "    df['user_nb_1st_reorder'] = df.user_id.map(users.nb_1st_reorder)\n",
    "    df['user_nb_2nd_reorder'] = df.user_id.map(users.nb_2nd_reorder)\n",
    "    df['user_nb_3rd_reorder'] = df.user_id.map(users.nb_3rd_reorder)\n",
    "    df['user_nb_4th_reorder'] = df.user_id.map(users.nb_4th_reorder)\n",
    "    df['user_nb_5th_reorder'] = df.user_id.map(users.nb_5th_reorder)\n",
    "    df['user_nb_6th_reorder'] = df.user_id.map(users.nb_6th_reorder)\n",
    "    df['user_nb_7th_reorder'] = df.user_id.map(users.nb_7th_reorder)\n",
    "    df['user_nb_8th_reorder'] = df.user_id.map(users.nb_8th_reorder)\n",
    "    df['user_nb_9th_reorder'] = df.user_id.map(users.nb_9th_reorder)\n",
    "    df['user_nb_10th_reorder'] = df.user_id.map(users.nb_10th_reorder)\n",
    "    \n",
    "    df['user_last_order_id'] = df.user_id.map(users.last_order_id)\n",
    "    df['user_lo_dow'] = df.user_last_order_id.map(order.order_dow)\n",
    "    df['user_lo_hour_of_day'] = df.user_last_order_id.map(order.order_hour_of_day)\n",
    "    df['user_lo_day_since_prior'] = df.user_last_order_id.map(order.days_since_prior_order)\n",
    "    df['user_lo_nb_products'] = df.user_id.map(users.lo_nb_products)\n",
    "    df['user_lo_first_item_id'] = df.user_id.map(users.lo_first_item_id)\n",
    "    df['user_lo_first_item_reorder'] = df.user_id.map(users.lo_first_item_reorder)\n",
    "    df['user_lo_nb_reorder'] = df.user_id.map(users.lo_nb_reorder)\n",
    "    df['user_lo_reorder_ratio'] = df.user_id.map(users.lo_reorder_ratio)\n",
    "    \n",
    "    df['user_last2_order_id'] = df.user_id.map(users.last_2_order_id)\n",
    "    df['user_lo2_dow'] = df.user_last2_order_id.map(order.order_dow)\n",
    "    df['user_lo2_hour_of_day'] = df.user_last2_order_id.map(order.order_hour_of_day)\n",
    "    df['user_lo2_day_since_prior'] = df.user_last2_order_id.map(order.days_since_prior_order)\n",
    "    df['user_lo2_nb_products'] = df.user_id.map(users.lo2_nb_products)\n",
    "    df['user_lo2_first_item_id'] = df.user_id.map(users.lo2_first_item_id)\n",
    "    df['user_lo2_first_item_reorder'] = df.user_id.map(users.lo2_first_item_reorder)\n",
    "    df['user_lo2_nb_reorder'] = df.user_id.map(users.lo2_nb_reorder)\n",
    "    df['user_lo2_reorder_ratio'] = df.user_id.map(users.lo2_reorder_ratio)\n",
    "    \n",
    "    df['user_last3_order_id'] = df.user_id.map(users.last_3_order_id)\n",
    "    df['user_lo3_dow'] = df.user_last3_order_id.map(order.order_dow)\n",
    "    df['user_lo3_hour_of_day'] = df.user_last3_order_id.map(order.order_hour_of_day)\n",
    "    df['user_lo3_day_since_prior'] = df.user_last3_order_id.map(order.days_since_prior_order)\n",
    "    df['user_lo3_nb_products'] = df.user_id.map(users.lo3_nb_products)\n",
    "    df['user_lo3_first_item_id'] = df.user_id.map(users.lo3_first_item_id)\n",
    "    df['user_lo3_first_item_reorder'] = df.user_id.map(users.lo3_first_item_reorder)\n",
    "    df['user_lo3_nb_reorder'] = df.user_id.map(users.lo3_nb_reorder)\n",
    "    df['user_lo3_reorder_ratio'] = df.user_id.map(users.lo3_reorder_ratio)\n",
    "    \n",
    "    df['users_min_basket'] = df.user_id.map(users.min_basket)\n",
    "    df['users_max_basket'] = df.user_id.map(users.max_basket)\n",
    "    df['users_avg_basket'] = df.user_id.map(users.avg_basket)\n",
    "    df['users_nb_unique_items'] = df.user_id.map(users.nb_unique_items)\n",
    "    df['users_nb_reorder_items'] = df.user_id.map(users.nb_reorder_items)\n",
    "    df['users_avg_reorder_per_basket'] = df.user_id.map(users.avg_reorder_per_basket)\n",
    "    df['users_reorder_order_vs_order_ratio'] = df.user_id.map(users.reorder_order_vs_order_ratio)\n",
    "    df['users_nb_all_items'] = df.user_id.map(users.nb_all_items)\n",
    "    df['users_nb_reorder_items_vs_nb_all_items_ratio'] = df.user_id.map(users.nb_reorder_items_vs_nb_all_items_ratio)\n",
    "    df['users_min_reorder_items'] = df.user_id.map(users.min_reorder_items)\n",
    "    df['users_max_reorder_items'] = df.user_id.map(users.max_reorder_items)\n",
    "    df['users_nb_organic_items'] = df.user_id.map(users.nb_organic_items)\n",
    "    df['users_organic_ratio'] = df.user_id.map(users.organic_ratio)\n",
    "    df['users_avg_organic_basket'] = df.user_id.map(users.avg_organic_basket)\n",
    "#     df['users_nb_organic_reordered'] = df.user_id.map(users.nb_organic_reordered)\n",
    "#     df['users_organic_reorder_ratio'] = df.user_id.map(users.organic_reorder_ratio)\n",
    "#     df['users_organic_reorder_ratio_VS_all'] = df.user_id.map(users.organic_reorder_ratio_VS_all)\n",
    "    \n",
    "    # department and aisle related features\n",
    "    df['users_nb_unique_departments'] = df.user_id.map(users.nb_unique_departments)\n",
    "    df['users_nb_unique_aisles'] = df.user_id.map(users.nb_unique_aisles)\n",
    "    df['users_favorite_department_id'] = df.user_id.map(users.favorite_department_id)\n",
    "    df['users_favorite_aisle_id'] = df.user_id.map(users.favorite_aisle_id)\n",
    "    df['users_nb_unique_reorder_departments'] = df.user_id.map(users.nb_unique_reorder_departments)\n",
    "    df['users_nb_unique_reorder_aisles'] = df.user_id.map(users.nb_unique_reorder_aisles)\n",
    "    df['users_favorite_reorder_department_id'] = df.user_id.map(users.favorite_reorder_department_id)\n",
    "    df['users_favorite_reorder_aisle_id'] = df.user_id.map(users.favorite_reorder_aisle_id)\n",
    "    df['users_depart_reorder_ratio'] = df.user_id.map(users.depart_reorder_ratio)\n",
    "    df['users_aisle_reorder_ratio'] = df.user_id.map(users.aisle_reorder_ratio)\n",
    "    \n",
    "    print(\"product related features<prior>\")\n",
    "    df['product_aisle_id'] = df.product_id.map(products.aisle_id)\n",
    "    df['product_department_id'] = df.product_id.map(products.department_id)\n",
    "    df['product_orders'] = df.product_id.map(products.total_nb)\n",
    "    df['product_reorders'] = df.product_id.map(products.nb_reorder)\n",
    "    df['product_reorder_rate'] = df.product_id.map(products.reorder_rate)\n",
    "    df['product_nb_buyers'] = df.product_id.map(products.nb_buyers)\n",
    "    df['product_avg_add_to_cart_order'] = df.product_id.map(products.avg_add_to_cart_order)\n",
    "    df['product_nb_orders'] = df.product_id.map(products.nb_orders)\n",
    "    df['product_min_add_to_cart_order'] = df.product_id.map(products.min_add_to_cart_order)\n",
    "    df['product_max_add_to_cart_order'] = df.product_id.map(products.max_add_to_cart_order)\n",
    "    \n",
    "    print(\"order related features<train>\")\n",
    "    df['order_hour_of_day'] = df.order_id.map(order.order_hour_of_day)\n",
    "    df['order_days_since_prior_order'] = df.order_id.map(order.days_since_prior_order)\n",
    "    df['order_day_of_week'] = df.order_id.map(order.order_dow)\n",
    "    df['order_number'] = df.order_id.map(order.order_number)\n",
    "#     df['order_nb_organic_items'] = df.order_id.map(ord_pri.nb_organic_items)\n",
    "    \n",
    "    print(\"userXproduct related features<prior>\")\n",
    "    # 1.nb_orders, 2.sum_add_to_cart_order, 3.nb_reordered, \\\n",
    "    # 4.last_order_id, 5.sum_order_dow, 6.sum_order_hour_of_day\n",
    "    df['UP'] = df.product_id+df.user_id.astype(np.uint64)*100000\n",
    "    df['UP_nb_orders'] = df.UP.map(UserProduct.nb_orders)\n",
    "    df['UP_avg_add_to_cart_order'] = df.UP.map(UserProduct.sum_add_to_cart_order)\\\n",
    "                                    / df.UP_nb_orders\n",
    "    df['UP_nb_reordered'] = df.UP.map(UserProduct.nb_reordered)\n",
    "    df['UP_reorder_ratio'] = (df.UP_nb_reordered / df.UP_nb_orders).astype(np.float32)\n",
    "    df['UP_last_order_id'] = df.UP.map(UserProduct.last_order_id)\n",
    "    df['UP_last_order_number'] = df.UP.map(UserProduct.last_order_number)\n",
    "    df['UP_first_order_id'] = df.UP.map(UserProduct.first_order_id)\n",
    "    df['UP_first_order_number'] = df.UP.map(UserProduct.first_order_number)\n",
    "    df['UP_avg_order_dow'] = (df.UP.map(UserProduct.sum_order_dow)\\\n",
    "                              / df.UP_nb_orders).astype(np.float32)\n",
    "    df['UP_avg_order_hour_of_day'] = (df.UP.map(UserProduct.sum_order_hour_of_day) / \\\n",
    "                                    df.UP_nb_orders).astype(np.float32)\n",
    "    df['UP_order_ratio'] = (df.UP_nb_orders / df.user_total_orders).astype(np.float32)\n",
    "    df['UP_order_since_last'] = df.user_total_orders - \\\n",
    "                                df.UP_last_order_id.map(order.order_number)\n",
    "    #最后一次买该产品和该订单-相同产品相隔的时间(没有算日期。。。)\n",
    "    df['UP_delta_hour_vs_last'] = abs(df.order_hour_of_day - df.UP_last_order_id.map(\\\n",
    "                                       order.order_hour_of_day)).map(lambda x: min(x, 24-x)).astype(np.int8)\n",
    "    df['UP_min_add_to_cart_order'] = df.UP.map(UserProduct.min_add_to_cart_order)\n",
    "    df['UP_max_add_to_cart_order'] = df.UP.map(UserProduct.max_add_to_cart_order)\n",
    "    df['UP_buy_first_time_total_nb'] = df.UP.map(UserProduct.buy_first_time_total_nb)\n",
    "    df['UP_buy_second_time_total_nb'] = df.UP.map(UserProduct.buy_second_time_total_nb)\n",
    "    df['UP_reorder_second_time_VS_first_time'] = df.UP.map(UserProduct.reorder_second_time_VS_first_time)\n",
    "    df['UP_nb_orders_since_last_order'] = df['user_total_orders'] - df['UP_last_order_number']\n",
    "    df['UP_order_rate'] = (df.UP_nb_orders / df.user_total_orders).astype(np.float32)\n",
    "    df.drop('UP', axis=1, inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(df.dtypes)\n",
    "#     print(df.memory_usage())\n",
    "    return(df, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train order size:  (131209, 7)\n",
      "generate features and labels(optional) from selected orders\n",
      "order row 10000\n",
      "order row 20000\n",
      "order row 30000\n",
      "order row 40000\n",
      "order row 50000\n",
      "order row 60000\n",
      "order row 70000\n",
      "order row 80000\n",
      "order row 90000\n",
      "order row 100000\n",
      "order row 110000\n",
      "order row 120000\n",
      "order row 130000\n",
      "user related features<prior>\n",
      "product related features<prior>\n",
      "order related features<train>\n",
      "userXproduct related features<prior>\n",
      "feature size:  (8531865, 115)\n"
     ]
    }
   ],
   "source": [
    "print('train order size: ', train_orders.shape)\n",
    "df_train, labels = gen_features(train_orders, labels_out=True)\n",
    "print('feature size: ', df_train.shape)\n",
    "df_train['label'] = pd.Series(labels, dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test order size:  (75000, 7)\n",
      "generate features and labels(optional) from selected orders\n",
      "order row 10000\n",
      "order row 20000\n",
      "order row 30000\n",
      "order row 40000\n",
      "order row 50000\n",
      "order row 60000\n",
      "order row 70000\n",
      "user related features<prior>\n",
      "product related features<prior>\n",
      "order related features<train>\n",
      "userXproduct related features<prior>\n"
     ]
    }
   ],
   "source": [
    "print('test order size: ', test_orders.shape)\n",
    "df_test, _ = gen_features(test_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del users\n",
    "del products\n",
    "del prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['label'] = pd.Series(labels, dtype=np.int8)\n",
    "user_id_list=users.index.tolist()\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(user_id_list)\n",
    "nb_user = len(user_id_list)\n",
    "val_nb_user = nb_user // 15\n",
    "# del users\n",
    "# del order\n",
    "# del UserProduct\n",
    "# del products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user size:  206209\n",
      "Doing Cross 0 Validation\n",
      "validation start: 0, end: 13747\n",
      "Split train and valid data/label by user_id\n",
      "[0]\ttrain-logloss:0.644913\tval-logloss:0.644843\n",
      "Multiple eval metrics have been passed: 'val-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-logloss hasn't improved in 100 rounds.\n",
      "[1]\ttrain-logloss:0.602998\tval-logloss:0.602871\n",
      "[2]\ttrain-logloss:0.566325\tval-logloss:0.566165\n",
      "[3]\ttrain-logloss:0.533992\tval-logloss:0.533797\n",
      "[4]\ttrain-logloss:0.505363\tval-logloss:0.505138\n",
      "[5]\ttrain-logloss:0.479911\tval-logloss:0.479665\n",
      "[6]\ttrain-logloss:0.457198\tval-logloss:0.456934\n",
      "[7]\ttrain-logloss:0.436865\tval-logloss:0.436593\n",
      "[8]\ttrain-logloss:0.418633\tval-logloss:0.418359\n",
      "[9]\ttrain-logloss:0.402235\tval-logloss:0.40195\n",
      "[10]\ttrain-logloss:0.387453\tval-logloss:0.38716\n",
      "[11]\ttrain-logloss:0.374111\tval-logloss:0.373825\n",
      "[12]\ttrain-logloss:0.362047\tval-logloss:0.361766\n",
      "[13]\ttrain-logloss:0.351142\tval-logloss:0.350879\n",
      "[14]\ttrain-logloss:0.341257\tval-logloss:0.341005\n",
      "[15]\ttrain-logloss:0.332303\tval-logloss:0.332057\n",
      "[16]\ttrain-logloss:0.324179\tval-logloss:0.323958\n",
      "[17]\ttrain-logloss:0.316797\tval-logloss:0.316605\n",
      "[18]\ttrain-logloss:0.31009\tval-logloss:0.309914\n",
      "[19]\ttrain-logloss:0.304005\tval-logloss:0.303838\n",
      "[20]\ttrain-logloss:0.298477\tval-logloss:0.298332\n",
      "[21]\ttrain-logloss:0.293446\tval-logloss:0.29333\n",
      "[22]\ttrain-logloss:0.288867\tval-logloss:0.288773\n",
      "[23]\ttrain-logloss:0.284699\tval-logloss:0.284622\n",
      "[24]\ttrain-logloss:0.280901\tval-logloss:0.280856\n",
      "[25]\ttrain-logloss:0.277458\tval-logloss:0.277437\n",
      "[26]\ttrain-logloss:0.274313\tval-logloss:0.274322\n",
      "[27]\ttrain-logloss:0.271456\tval-logloss:0.271501\n",
      "[28]\ttrain-logloss:0.26884\tval-logloss:0.26891\n",
      "[29]\ttrain-logloss:0.266464\tval-logloss:0.266573\n",
      "[30]\ttrain-logloss:0.264302\tval-logloss:0.264439\n",
      "[31]\ttrain-logloss:0.262337\tval-logloss:0.262505\n",
      "[32]\ttrain-logloss:0.26055\tval-logloss:0.260751\n",
      "[33]\ttrain-logloss:0.258907\tval-logloss:0.259135\n",
      "[34]\ttrain-logloss:0.257426\tval-logloss:0.257695\n",
      "[35]\ttrain-logloss:0.256067\tval-logloss:0.256383\n",
      "[36]\ttrain-logloss:0.254832\tval-logloss:0.255189\n",
      "[37]\ttrain-logloss:0.2537\tval-logloss:0.2541\n",
      "[38]\ttrain-logloss:0.252669\tval-logloss:0.253112\n",
      "[39]\ttrain-logloss:0.251727\tval-logloss:0.252216\n",
      "[40]\ttrain-logloss:0.250867\tval-logloss:0.251399\n",
      "[41]\ttrain-logloss:0.250086\tval-logloss:0.250663\n",
      "[42]\ttrain-logloss:0.249364\tval-logloss:0.249986\n",
      "[43]\ttrain-logloss:0.248712\tval-logloss:0.249377\n",
      "[44]\ttrain-logloss:0.248113\tval-logloss:0.248827\n",
      "[45]\ttrain-logloss:0.247562\tval-logloss:0.248329\n",
      "[46]\ttrain-logloss:0.247056\tval-logloss:0.247877\n",
      "[47]\ttrain-logloss:0.246596\tval-logloss:0.247462\n",
      "[48]\ttrain-logloss:0.246163\tval-logloss:0.247079\n",
      "[49]\ttrain-logloss:0.245773\tval-logloss:0.246743\n",
      "[50]\ttrain-logloss:0.245421\tval-logloss:0.246439\n",
      "[51]\ttrain-logloss:0.245098\tval-logloss:0.246159\n",
      "[52]\ttrain-logloss:0.244782\tval-logloss:0.245895\n",
      "[53]\ttrain-logloss:0.244499\tval-logloss:0.245665\n",
      "[54]\ttrain-logloss:0.244243\tval-logloss:0.24546\n",
      "[55]\ttrain-logloss:0.244001\tval-logloss:0.245265\n",
      "[56]\ttrain-logloss:0.243767\tval-logloss:0.245093\n",
      "[57]\ttrain-logloss:0.243547\tval-logloss:0.244922\n",
      "[58]\ttrain-logloss:0.243341\tval-logloss:0.244774\n",
      "[59]\ttrain-logloss:0.243158\tval-logloss:0.244639\n",
      "[60]\ttrain-logloss:0.24298\tval-logloss:0.244515\n",
      "[61]\ttrain-logloss:0.242821\tval-logloss:0.244402\n",
      "[62]\ttrain-logloss:0.242669\tval-logloss:0.244309\n",
      "[63]\ttrain-logloss:0.242521\tval-logloss:0.244215\n",
      "[64]\ttrain-logloss:0.242371\tval-logloss:0.244122\n",
      "[65]\ttrain-logloss:0.242237\tval-logloss:0.24404\n",
      "[66]\ttrain-logloss:0.242122\tval-logloss:0.243964\n",
      "[67]\ttrain-logloss:0.242001\tval-logloss:0.243907\n",
      "[68]\ttrain-logloss:0.241876\tval-logloss:0.243844\n",
      "[69]\ttrain-logloss:0.241752\tval-logloss:0.243783\n",
      "[70]\ttrain-logloss:0.241653\tval-logloss:0.243739\n",
      "[71]\ttrain-logloss:0.241558\tval-logloss:0.2437\n",
      "[72]\ttrain-logloss:0.241451\tval-logloss:0.243636\n",
      "[73]\ttrain-logloss:0.241343\tval-logloss:0.243592\n",
      "[74]\ttrain-logloss:0.241256\tval-logloss:0.243556\n",
      "[75]\ttrain-logloss:0.241164\tval-logloss:0.243515\n",
      "[76]\ttrain-logloss:0.241082\tval-logloss:0.243487\n",
      "[77]\ttrain-logloss:0.240992\tval-logloss:0.243452\n",
      "[78]\ttrain-logloss:0.240898\tval-logloss:0.243425\n",
      "[79]\ttrain-logloss:0.240807\tval-logloss:0.243406\n",
      "[80]\ttrain-logloss:0.240715\tval-logloss:0.243382\n",
      "[81]\ttrain-logloss:0.240634\tval-logloss:0.243363\n",
      "[82]\ttrain-logloss:0.240548\tval-logloss:0.243336\n",
      "[83]\ttrain-logloss:0.240464\tval-logloss:0.243303\n",
      "[84]\ttrain-logloss:0.240392\tval-logloss:0.243275\n",
      "[85]\ttrain-logloss:0.240316\tval-logloss:0.243252\n",
      "[86]\ttrain-logloss:0.240228\tval-logloss:0.243232\n",
      "[87]\ttrain-logloss:0.240153\tval-logloss:0.243211\n",
      "[88]\ttrain-logloss:0.240083\tval-logloss:0.243198\n",
      "[89]\ttrain-logloss:0.240006\tval-logloss:0.243192\n",
      "[90]\ttrain-logloss:0.239923\tval-logloss:0.243173\n",
      "[91]\ttrain-logloss:0.239843\tval-logloss:0.24315\n",
      "[92]\ttrain-logloss:0.23975\tval-logloss:0.243141\n",
      "[93]\ttrain-logloss:0.23969\tval-logloss:0.24313\n",
      "[94]\ttrain-logloss:0.239625\tval-logloss:0.243131\n",
      "[95]\ttrain-logloss:0.239568\tval-logloss:0.24312\n",
      "[96]\ttrain-logloss:0.239494\tval-logloss:0.243097\n",
      "[97]\ttrain-logloss:0.239452\tval-logloss:0.243088\n",
      "[98]\ttrain-logloss:0.239401\tval-logloss:0.243086\n",
      "[99]\ttrain-logloss:0.239323\tval-logloss:0.243075\n",
      "[100]\ttrain-logloss:0.239266\tval-logloss:0.243067\n",
      "[101]\ttrain-logloss:0.239198\tval-logloss:0.24306\n",
      "[102]\ttrain-logloss:0.239138\tval-logloss:0.243058\n",
      "[103]\ttrain-logloss:0.23908\tval-logloss:0.243046\n",
      "[104]\ttrain-logloss:0.238998\tval-logloss:0.24303\n",
      "[105]\ttrain-logloss:0.238921\tval-logloss:0.243014\n",
      "[106]\ttrain-logloss:0.238852\tval-logloss:0.243006\n",
      "[107]\ttrain-logloss:0.238803\tval-logloss:0.242997\n",
      "[108]\ttrain-logloss:0.238721\tval-logloss:0.242988\n",
      "[109]\ttrain-logloss:0.238638\tval-logloss:0.242979\n",
      "[110]\ttrain-logloss:0.238573\tval-logloss:0.24297\n",
      "[111]\ttrain-logloss:0.238514\tval-logloss:0.242967\n",
      "[112]\ttrain-logloss:0.23846\tval-logloss:0.242961\n",
      "[113]\ttrain-logloss:0.238401\tval-logloss:0.242946\n",
      "[114]\ttrain-logloss:0.238329\tval-logloss:0.242939\n",
      "[115]\ttrain-logloss:0.238275\tval-logloss:0.242937\n",
      "[116]\ttrain-logloss:0.238211\tval-logloss:0.242928\n",
      "[117]\ttrain-logloss:0.238144\tval-logloss:0.242922\n",
      "[118]\ttrain-logloss:0.238098\tval-logloss:0.242914\n",
      "[119]\ttrain-logloss:0.238033\tval-logloss:0.242907\n",
      "[120]\ttrain-logloss:0.23797\tval-logloss:0.242906\n",
      "[121]\ttrain-logloss:0.237916\tval-logloss:0.242909\n",
      "[122]\ttrain-logloss:0.23786\tval-logloss:0.242902\n",
      "[123]\ttrain-logloss:0.237822\tval-logloss:0.24289\n",
      "[124]\ttrain-logloss:0.237779\tval-logloss:0.242883\n",
      "[125]\ttrain-logloss:0.237708\tval-logloss:0.242882\n",
      "[126]\ttrain-logloss:0.23767\tval-logloss:0.242883\n",
      "[127]\ttrain-logloss:0.237638\tval-logloss:0.242877\n",
      "[128]\ttrain-logloss:0.237579\tval-logloss:0.242878\n",
      "[129]\ttrain-logloss:0.237528\tval-logloss:0.242876\n",
      "[130]\ttrain-logloss:0.237445\tval-logloss:0.242879\n",
      "[131]\ttrain-logloss:0.237373\tval-logloss:0.242881\n",
      "[132]\ttrain-logloss:0.237317\tval-logloss:0.242868\n",
      "[133]\ttrain-logloss:0.23727\tval-logloss:0.242863\n",
      "[134]\ttrain-logloss:0.237214\tval-logloss:0.242851\n",
      "[135]\ttrain-logloss:0.237151\tval-logloss:0.242854\n",
      "[136]\ttrain-logloss:0.23708\tval-logloss:0.24285\n",
      "[137]\ttrain-logloss:0.237001\tval-logloss:0.242853\n",
      "[138]\ttrain-logloss:0.236965\tval-logloss:0.242858\n",
      "[139]\ttrain-logloss:0.23689\tval-logloss:0.242866\n",
      "[140]\ttrain-logloss:0.236871\tval-logloss:0.242869\n",
      "[141]\ttrain-logloss:0.236811\tval-logloss:0.242855\n",
      "[142]\ttrain-logloss:0.236728\tval-logloss:0.24286\n",
      "[143]\ttrain-logloss:0.236682\tval-logloss:0.242855\n",
      "[144]\ttrain-logloss:0.236664\tval-logloss:0.24285\n",
      "[145]\ttrain-logloss:0.236594\tval-logloss:0.242855\n",
      "[146]\ttrain-logloss:0.236548\tval-logloss:0.242852\n",
      "[147]\ttrain-logloss:0.236499\tval-logloss:0.242846\n",
      "[148]\ttrain-logloss:0.236443\tval-logloss:0.242849\n",
      "[149]\ttrain-logloss:0.236411\tval-logloss:0.242841\n",
      "[150]\ttrain-logloss:0.236365\tval-logloss:0.242838\n",
      "[151]\ttrain-logloss:0.236333\tval-logloss:0.242835\n",
      "[152]\ttrain-logloss:0.236292\tval-logloss:0.242829\n",
      "[153]\ttrain-logloss:0.236237\tval-logloss:0.242827\n",
      "[154]\ttrain-logloss:0.236179\tval-logloss:0.242826\n",
      "[155]\ttrain-logloss:0.23612\tval-logloss:0.242835\n",
      "[156]\ttrain-logloss:0.23605\tval-logloss:0.242834\n",
      "[157]\ttrain-logloss:0.236014\tval-logloss:0.242829\n",
      "[158]\ttrain-logloss:0.235971\tval-logloss:0.24283\n",
      "[159]\ttrain-logloss:0.235925\tval-logloss:0.242825\n",
      "[160]\ttrain-logloss:0.235867\tval-logloss:0.242824\n",
      "[161]\ttrain-logloss:0.23581\tval-logloss:0.242823\n",
      "[162]\ttrain-logloss:0.235749\tval-logloss:0.242822\n",
      "[163]\ttrain-logloss:0.235698\tval-logloss:0.242806\n",
      "[164]\ttrain-logloss:0.235671\tval-logloss:0.242794\n",
      "[165]\ttrain-logloss:0.235636\tval-logloss:0.242792\n",
      "[166]\ttrain-logloss:0.235625\tval-logloss:0.242791\n",
      "[167]\ttrain-logloss:0.23559\tval-logloss:0.242786\n",
      "[168]\ttrain-logloss:0.235522\tval-logloss:0.24278\n",
      "[169]\ttrain-logloss:0.235478\tval-logloss:0.24278\n",
      "[170]\ttrain-logloss:0.235459\tval-logloss:0.242778\n",
      "[171]\ttrain-logloss:0.235444\tval-logloss:0.242776\n",
      "[172]\ttrain-logloss:0.235385\tval-logloss:0.242776\n",
      "[173]\ttrain-logloss:0.235315\tval-logloss:0.242779\n",
      "[174]\ttrain-logloss:0.235273\tval-logloss:0.242782\n",
      "[175]\ttrain-logloss:0.235195\tval-logloss:0.242791\n",
      "[176]\ttrain-logloss:0.235152\tval-logloss:0.24279\n",
      "[177]\ttrain-logloss:0.235115\tval-logloss:0.242785\n",
      "[178]\ttrain-logloss:0.235069\tval-logloss:0.242782\n",
      "[179]\ttrain-logloss:0.235012\tval-logloss:0.242769\n",
      "[180]\ttrain-logloss:0.234956\tval-logloss:0.242767\n",
      "[181]\ttrain-logloss:0.234922\tval-logloss:0.242767\n",
      "[182]\ttrain-logloss:0.234889\tval-logloss:0.242769\n",
      "[183]\ttrain-logloss:0.234843\tval-logloss:0.242761\n",
      "[184]\ttrain-logloss:0.234808\tval-logloss:0.242758\n",
      "[185]\ttrain-logloss:0.234759\tval-logloss:0.242759\n",
      "[186]\ttrain-logloss:0.234705\tval-logloss:0.242765\n",
      "[187]\ttrain-logloss:0.234647\tval-logloss:0.242761\n",
      "[188]\ttrain-logloss:0.234601\tval-logloss:0.242753\n",
      "[189]\ttrain-logloss:0.234548\tval-logloss:0.242744\n",
      "[190]\ttrain-logloss:0.234519\tval-logloss:0.242745\n",
      "[191]\ttrain-logloss:0.23448\tval-logloss:0.242742\n",
      "[192]\ttrain-logloss:0.234433\tval-logloss:0.242746\n",
      "[193]\ttrain-logloss:0.234373\tval-logloss:0.24275\n",
      "[194]\ttrain-logloss:0.234319\tval-logloss:0.242743\n",
      "[195]\ttrain-logloss:0.234258\tval-logloss:0.242739\n",
      "[196]\ttrain-logloss:0.234232\tval-logloss:0.242738\n",
      "[197]\ttrain-logloss:0.234168\tval-logloss:0.242735\n",
      "[198]\ttrain-logloss:0.234119\tval-logloss:0.242735\n",
      "[199]\ttrain-logloss:0.234094\tval-logloss:0.242735\n",
      "[200]\ttrain-logloss:0.234048\tval-logloss:0.242731\n",
      "[201]\ttrain-logloss:0.233987\tval-logloss:0.242732\n",
      "[202]\ttrain-logloss:0.23395\tval-logloss:0.242729\n",
      "[203]\ttrain-logloss:0.233895\tval-logloss:0.242724\n",
      "[204]\ttrain-logloss:0.233846\tval-logloss:0.242722\n",
      "[205]\ttrain-logloss:0.23381\tval-logloss:0.242719\n",
      "[206]\ttrain-logloss:0.233771\tval-logloss:0.242713\n",
      "[207]\ttrain-logloss:0.233723\tval-logloss:0.242707\n",
      "[208]\ttrain-logloss:0.233672\tval-logloss:0.242712\n",
      "[209]\ttrain-logloss:0.233648\tval-logloss:0.242711\n",
      "[210]\ttrain-logloss:0.233622\tval-logloss:0.242707\n",
      "[211]\ttrain-logloss:0.233558\tval-logloss:0.242699\n",
      "[212]\ttrain-logloss:0.233515\tval-logloss:0.242695\n",
      "[213]\ttrain-logloss:0.233463\tval-logloss:0.242692\n",
      "[214]\ttrain-logloss:0.233402\tval-logloss:0.242685\n",
      "[215]\ttrain-logloss:0.233359\tval-logloss:0.24268\n",
      "[216]\ttrain-logloss:0.233326\tval-logloss:0.242676\n",
      "[217]\ttrain-logloss:0.233299\tval-logloss:0.242677\n",
      "[218]\ttrain-logloss:0.233265\tval-logloss:0.242675\n",
      "[219]\ttrain-logloss:0.233245\tval-logloss:0.242679\n",
      "[220]\ttrain-logloss:0.233193\tval-logloss:0.242671\n",
      "[221]\ttrain-logloss:0.233172\tval-logloss:0.242674\n",
      "[222]\ttrain-logloss:0.233153\tval-logloss:0.242677\n",
      "[223]\ttrain-logloss:0.233111\tval-logloss:0.242679\n",
      "[224]\ttrain-logloss:0.233055\tval-logloss:0.242687\n",
      "[225]\ttrain-logloss:0.232998\tval-logloss:0.24269\n",
      "[226]\ttrain-logloss:0.232972\tval-logloss:0.242685\n",
      "[227]\ttrain-logloss:0.232928\tval-logloss:0.242687\n",
      "[228]\ttrain-logloss:0.23289\tval-logloss:0.242688\n",
      "[229]\ttrain-logloss:0.232822\tval-logloss:0.242691\n",
      "[230]\ttrain-logloss:0.232788\tval-logloss:0.242684\n",
      "[231]\ttrain-logloss:0.232727\tval-logloss:0.242685\n",
      "[232]\ttrain-logloss:0.232671\tval-logloss:0.24269\n",
      "[233]\ttrain-logloss:0.232644\tval-logloss:0.242684\n",
      "[234]\ttrain-logloss:0.232585\tval-logloss:0.242671\n",
      "[235]\ttrain-logloss:0.232524\tval-logloss:0.242681\n",
      "[236]\ttrain-logloss:0.232497\tval-logloss:0.242677\n",
      "[237]\ttrain-logloss:0.232451\tval-logloss:0.24268\n",
      "[238]\ttrain-logloss:0.232396\tval-logloss:0.242687\n",
      "[239]\ttrain-logloss:0.232353\tval-logloss:0.242682\n",
      "[240]\ttrain-logloss:0.232325\tval-logloss:0.24268\n",
      "[241]\ttrain-logloss:0.232279\tval-logloss:0.242672\n",
      "[242]\ttrain-logloss:0.232218\tval-logloss:0.242672\n",
      "[243]\ttrain-logloss:0.232188\tval-logloss:0.242668\n",
      "[244]\ttrain-logloss:0.232138\tval-logloss:0.242671\n",
      "[245]\ttrain-logloss:0.232107\tval-logloss:0.242675\n",
      "[246]\ttrain-logloss:0.232064\tval-logloss:0.242674\n",
      "[247]\ttrain-logloss:0.232012\tval-logloss:0.242671\n",
      "[248]\ttrain-logloss:0.231976\tval-logloss:0.242674\n",
      "[249]\ttrain-logloss:0.231944\tval-logloss:0.242674\n"
     ]
    }
   ],
   "source": [
    "print(\"user size: \", nb_user)\n",
    "for nb in range(1):\n",
    "    print(\"Doing Cross %d Validation\"% nb)\n",
    "    val_start = nb*val_nb_user\n",
    "    val_end = (nb+1)*val_nb_user\n",
    "    if nb==14:\n",
    "        val_end = nb_user\n",
    "    print(\"validation start: %d, end: %d\" % (val_start, val_end))\n",
    "    train_user_ids = np.concatenate((user_id_list[:val_start],\\\n",
    "                                    user_id_list[val_end:]))\n",
    "    val_user_ids = user_id_list[val_start:val_end]\n",
    "    print(\"Split train and valid data/label by user_id\")\n",
    "    sub_df_val = df_train[df_train.user_id.isin(val_user_ids)].copy()\n",
    "    sub_df_train = df_train[df_train.user_id.isin(train_user_ids)].copy()\n",
    "    sub_train_label = np.array(sub_df_train['label'])\n",
    "    sub_val_label = np.array(sub_df_val['label'])\n",
    "    sub_df_train.drop(['label'], axis=1, inplace=True)\n",
    "    sub_df_val.drop(['label'], axis=1, inplace=True)\n",
    "    params={\n",
    "    'objective': 'reg:logistic', \n",
    "    'eval_metric': 'logloss',\n",
    "    'gamma':0.7,  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。\n",
    "    'max_depth':10, # 构建树的深度，越大越容易过拟合\n",
    "    'lambda':10,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "    'subsample':0.76, # 随机采样训练样本\n",
    "    'colsample_bytree':0.95, # 生成树时进行的列采样\n",
    "    'min_child_weight':10,  \n",
    "    'silent':0 ,#设置成1则没有运行信息输出，最好是设置为0.\n",
    "    'eta': 0.07, # 如同学习率\n",
    "    'nthread':8,# cpu 线程数\n",
    "    }\n",
    "    train = np.array(sub_df_train)\n",
    "    valid = np.array(sub_df_val)\n",
    "    n = 250\n",
    "    plst = list(params.items())\n",
    "    xgtrain = xgb.DMatrix(train, label=sub_train_label)\n",
    "    xgval = xgb.DMatrix(valid, label=sub_val_label)\n",
    "    watchlist = [(xgtrain, 'train'), (xgval, 'val')]\n",
    "    model = xgb.train(plst, xgtrain, n, watchlist, early_stopping_rounds=100)\n",
    "    model.save_model('CV_0814_'+str(nb)+'.model')\n",
    "    del sub_df_train\n",
    "    del sub_df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost predict from model 0\n"
     ]
    }
   ],
   "source": [
    "df_test_array = np.array(df_test)\n",
    "xgtest = xgb.DMatrix(df_test_array)\n",
    "xgbst = xgb.Booster()\n",
    "for i in range(1):\n",
    "    print('xgboost predict from model',str(i))\n",
    "    xgbst.load_model('./CV_0814_'+str(i)+'.model')\n",
    "    if i == 0:\n",
    "        preds = xgbst.predict(xgtest)\n",
    "    else:\n",
    "        preds += xgbst.predict(xgtest)\n",
    "# preds = preds / 15.\n",
    "df_test['preds']=preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "THRESHOLD=0.2\n",
    "d = dict()\n",
    "for row in df_test.itertuples():\n",
    "    if row.preds > THRESHOLD:\n",
    "        try:\n",
    "            d[row.order_id] += ' ' + str(row.product_id)\n",
    "        except:\n",
    "            d[row.order_id] = str(row.product_id)\n",
    "for order in test_orders.order_id:\n",
    "    if order not in d:\n",
    "        d[order] = 'None'\n",
    "\n",
    "tst = pd.DataFrame.from_dict(d, orient='index')\n",
    "tst.reset_index(inplace=True)\n",
    "tst.columns = ['order_id', 'products']\n",
    "tst.to_csv('submission_82_features_THRESHOLD_0.2_15_fold_CV_0809.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: Faron\n",
    "\"\"\"\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pylab as plt\n",
    "from datetime import datetime\n",
    "\n",
    "'''\n",
    "This kernel implements the O(n²) F1-Score expectation maximization algorithm presented in\n",
    "\"Ye, N., Chai, K., Lee, W., and Chieu, H.  Optimizing F-measures: A Tale of Two Approaches. In ICML, 2012.\"\n",
    "\n",
    "It solves argmax_(0 <= k <= n,[[None]]) E[F1(P,k,[[None]])]\n",
    "with [[None]] being the indicator for predicting label \"None\"\n",
    "given posteriors P = [p_1, p_2, ... , p_n], where p_1 > p_2 > ... > p_n\n",
    "under label independence assumption by means of dynamic programming in O(n²).\n",
    "'''\n",
    "\n",
    "\n",
    "class F1Optimizer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def get_expectations(P, pNone=None):\n",
    "        expectations = []\n",
    "        P = np.sort(P)[::-1]\n",
    "\n",
    "        n = np.array(P).shape[0]\n",
    "        DP_C = np.zeros((n + 2, n + 1))\n",
    "        if pNone is None:\n",
    "            pNone = (1.0 - P).prod()\n",
    "\n",
    "        DP_C[0][0] = 1.0\n",
    "        for j in range(1, n):\n",
    "            DP_C[0][j] = (1.0 - P[j - 1]) * DP_C[0, j - 1]\n",
    "\n",
    "        for i in range(1, n + 1):\n",
    "            DP_C[i, i] = DP_C[i - 1, i - 1] * P[i - 1]\n",
    "            for j in range(i + 1, n + 1):\n",
    "                DP_C[i, j] = P[j - 1] * DP_C[i - 1, j - 1] + (1.0 - P[j - 1]) * DP_C[i, j - 1]\n",
    "\n",
    "        DP_S = np.zeros((2 * n + 1,))\n",
    "        DP_SNone = np.zeros((2 * n + 1,))\n",
    "        for i in range(1, 2 * n + 1):\n",
    "            DP_S[i] = 1. / (1. * i)\n",
    "            DP_SNone[i] = 1. / (1. * i + 1)\n",
    "        for k in range(n + 1)[::-1]:\n",
    "            f1 = 0\n",
    "            f1None = 0\n",
    "            for k1 in range(n + 1):\n",
    "                f1 += 2 * k1 * DP_C[k1][k] * DP_S[k + k1]\n",
    "                f1None += 2 * k1 * DP_C[k1][k] * DP_SNone[k + k1]\n",
    "            for i in range(1, 2 * k - 1):\n",
    "                DP_S[i] = (1 - P[k - 1]) * DP_S[i] + P[k - 1] * DP_S[i + 1]\n",
    "                DP_SNone[i] = (1 - P[k - 1]) * DP_SNone[i] + P[k - 1] * DP_SNone[i + 1]\n",
    "            expectations.append([f1None + 2 * pNone / (2 + k), f1])\n",
    "\n",
    "        return np.array(expectations[::-1]).T\n",
    "\n",
    "    @staticmethod\n",
    "    def maximize_expectation(P, pNone=None):\n",
    "        expectations = F1Optimizer.get_expectations(P, pNone)\n",
    "\n",
    "        ix_max = np.unravel_index(expectations.argmax(), expectations.shape)\n",
    "        max_f1 = expectations[ix_max]\n",
    "\n",
    "        predNone = True if ix_max[0] == 0 else False\n",
    "        best_k = ix_max[1]\n",
    "\n",
    "        return best_k, predNone, max_f1\n",
    "\n",
    "    @staticmethod\n",
    "    def _F1(tp, fp, fn):\n",
    "        return 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def _Fbeta(tp, fp, fn, beta=1.0):\n",
    "        beta_squared = beta ** 2\n",
    "        return (1.0 + beta_squared) * tp / ((1.0 + beta_squared) * tp + fp + beta_squared * fn)\n",
    "\n",
    "\n",
    "def print_best_prediction(P, pNone=None):\n",
    "    print(\"Maximize F1-Expectation\")\n",
    "    print(\"=\" * 23)\n",
    "    P = np.sort(P)[::-1]\n",
    "    n = P.shape[0]\n",
    "    L = ['L{}'.format(i + 1) for i in range(n)]\n",
    "\n",
    "    if pNone is None:\n",
    "        print(\"Estimate p(None|x) as (1-p_1)*(1-p_2)*...*(1-p_n)\")\n",
    "        pNone = (1.0 - P).prod()\n",
    "\n",
    "    PL = ['p({}|x)={}'.format(l, p) for l, p in zip(L, P)]\n",
    "    print(\"Posteriors: {} (n={})\".format(PL, n))\n",
    "    print(\"p(None|x)={}\".format(pNone))\n",
    "\n",
    "    opt = F1Optimizer.maximize_expectation(P, pNone)\n",
    "    best_prediction = ['None'] if opt[1] else []\n",
    "    best_prediction += (L[:opt[0]])\n",
    "    f1_max = opt[2]\n",
    "\n",
    "    print(\"Prediction {} yields best E[F1] of {}\\n\".format(best_prediction, f1_max))\n",
    "\n",
    "\n",
    "def save_plot(P, filename='expected_f1.png'):\n",
    "    E_F1 = pd.DataFrame(F1Optimizer.get_expectations(P).T, columns=[\"/w None\", \"/wo None\"])\n",
    "    best_k, _, max_f1 = F1Optimizer.maximize_expectation(P)\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    E_F1.plot()\n",
    "    plt.title('Expected F1-Score for \\n {}'.format(\"P = [{}]\".format(\",\".join(map(str, P)))), fontsize=12)\n",
    "    plt.xlabel('k')\n",
    "    plt.xticks(np.arange(0, len(P) + 1, 1.0))\n",
    "    plt.ylabel('E[F1(P,k)]')\n",
    "    plt.plot([best_k], [max_f1], 'o', color='#000000', markersize=4)\n",
    "    plt.annotate('max E[F1(P,k)] = E[F1(P,{})] = {:.5f}'.format(best_k, max_f1), xy=(best_k, max_f1),\n",
    "                 xytext=(best_k, max_f1 * 0.8), arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=7),\n",
    "                 horizontalalignment='center', verticalalignment='top')\n",
    "    plt.gcf().savefig(filename)\n",
    "\n",
    "\n",
    "\n",
    "def timeit(P):\n",
    "    s = datetime.now()\n",
    "    F1Optimizer.maximize_expectation(P)\n",
    "    e = datetime.now()\n",
    "    return (e-s).microseconds / 1E6\n",
    "\n",
    "\n",
    "def benchmark(n=100, filename='runtimes.png'):\n",
    "    results = pd.DataFrame(index=np.arange(1,n+1))\n",
    "    results['runtimes'] = 0\n",
    "\n",
    "    for i in range(1,n+1):\n",
    "        runtimes = []\n",
    "        for j in range(5):\n",
    "            runtimes.append(timeit(np.sort(np.random.rand(i))[::-1]))\n",
    "        results.iloc[i-1] = np.mean(runtimes)\n",
    "\n",
    "    x = results.index\n",
    "    y = results.runtimes\n",
    "    results['quadratic fit'] = np.poly1d(np.polyfit(x, y, deg=2))(x)\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    results.plot()\n",
    "    plt.title('Expectation Maximization Runtimes', fontsize=12)\n",
    "    plt.xlabel('n = |P|')\n",
    "    plt.ylabel('time in seconds')\n",
    "    plt.gcf().savefig(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # maximize f1-score\n",
    "# 49689-->None\n",
    "# xgbst = xgb.Booster()\n",
    "# xgbst.load_model('./CV_0807_0.model')\n",
    "# pred_result = pd.read_csv('./82_features_THRESHOLD_0.2_15_fold_CV_0809_pred_prob.csv')\n",
    "# display(pred_result)\n",
    "pred_result = pd.DataFrame()\n",
    "pred_result['order_id'] = df_test.groupby('order_id')['order_id'].apply(lambda x: x.iloc[0])\n",
    "pred_result['product_id'] = df_test.groupby('order_id')['product_id'].apply(list)\n",
    "pred_result['prob'] = df_test.groupby('order_id')['preds'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count=10000, order_id=454796\n",
      "count=20000, order_id=910889\n",
      "count=30000, order_id=1369147\n",
      "count=40000, order_id=1824309\n",
      "count=50000, order_id=2279414\n",
      "count=60000, order_id=2736272\n",
      "count=70000, order_id=3187737\n"
     ]
    }
   ],
   "source": [
    "basket_size = []\n",
    "count = 0\n",
    "pred_list=dict()\n",
    "pred_none=None\n",
    "pred_result.set_index('order_id', inplace=True, drop=False)\n",
    "for order_id in pred_result['order_id']:\n",
    "    count+=1\n",
    "    if not count % 10000:\n",
    "        print(\"count=%d, order_id=%d\" % (count, order_id))\n",
    "    product_id = pred_result.product_id[order_id]\n",
    "    prob = pred_result.prob[order_id]\n",
    "    d = dict(zip(product_id, prob))\n",
    "    sorted_keys = sorted(d, key=lambda x: d[x])[::-1]\n",
    "    if '49689' in product_id: # Wierd: Use string here\n",
    "        for i, prod_id in enumerate(product_id):\n",
    "            if prod_id==49689: # find index\n",
    "                pred_none = prob[i]\n",
    "                np.delete(prob, i, axis=0)\n",
    "                np.delete(product_id, i, axis=0)\n",
    "    else:\n",
    "        pred_none = None\n",
    "    opt = F1Optimizer.maximize_expectation(np.array(prob), pred_none)\n",
    "    if opt[1]:\n",
    "        pred_list[order_id]='None'\n",
    "    else:\n",
    "        if opt[0] == 0:\n",
    "            pred_list[order_id]='None'\n",
    "        else:\n",
    "            for choose_key in sorted_keys[:opt[0]]:\n",
    "                if choose_key == 49689:\n",
    "#                     pred_list[order_id] = 'None'\n",
    "#                     break\n",
    "                    try:\n",
    "                        pred_list[order_id] += ' None'\n",
    "                    except:\n",
    "                        pred_list[order_id] = 'None'\n",
    "                else:\n",
    "                    try:\n",
    "                        pred_list[order_id] += ' '+str(choose_key)\n",
    "                    except:\n",
    "                        pred_list[order_id] =str(choose_key)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst = pd.DataFrame.from_dict(pred_list, orient='index')\n",
    "tst.reset_index(inplace=True)\n",
    "tst.columns = ['order_id', 'products']\n",
    "tst.to_csv('submission_115_features_dynamic_THRESHOLD_1_fold_CV_0814_with_none.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

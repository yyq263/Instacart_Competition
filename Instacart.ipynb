{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading aisles...\n",
      "loading department...\n",
      "loading products...\n",
      "loading prior orders...\n",
      "loading train orders...\n",
      "loading orders...\n",
      "loading None products in prior\n",
      "loading None products in train\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "%matplotlib inline\n",
    "\n",
    "print('loading aisles...')\n",
    "aisles = pd.read_csv('aisles_.csv', dtype={\n",
    "        'aisle_id': np.uint16,\n",
    "        'aisle': 'category'})\n",
    "\n",
    "print('loading department...')\n",
    "department = pd.read_csv('departments_.csv', dtype={\n",
    "            'department_id': np.uint8,\n",
    "            'department': 'category'})\n",
    "\n",
    "print('loading products...')\n",
    "products = pd.read_csv('products_.csv', dtype={\n",
    "        'product_id': np.uint16,\n",
    "        'order_id': np.uint32,\n",
    "        'aisle_id': np.uint8,\n",
    "        'department_id': np.uint8})\n",
    "\n",
    "print('loading prior orders...')\n",
    "prior = pd.read_csv('order_products__prior_.csv', dtype={\n",
    "        'order_id': np.uint32,\n",
    "        'product_id': np.uint16,\n",
    "        'add_to_cart_order': np.uint16,\n",
    "        'reordered': np.uint16})\n",
    "\n",
    "print('loading train orders...')\n",
    "train = pd.read_csv('order_products__train_.csv', dtype={\n",
    "        'order_id': np.uint32,\n",
    "        'product_id': np.uint16,\n",
    "        'add_to_cart_order': np.uint16,\n",
    "        'reordered': np.uint8})\n",
    "\n",
    "print('loading orders...')\n",
    "order = pd.read_csv('orders_.csv' , dtype={\n",
    "        'order_id': np.uint32,\n",
    "        'user_id': np.uint32,\n",
    "        'eval_set': 'category',\n",
    "        'order_number': np.uint16,\n",
    "        'order_dow': np.uint16,\n",
    "        'order_hour_of_day': np.uint16,\n",
    "        'days_since_prior_order': np.float32})\n",
    "\n",
    "print('loading None products in prior')\n",
    "prior_None = pd.read_csv('None_order_prior_.csv', dtype={\n",
    "            'order_id': np.uint32,\n",
    "            'product_id': np.uint16,\n",
    "            'add_to_cart_order': np.uint16,\n",
    "            'reordered': np.uint16})\n",
    "\n",
    "print('loading None products in train')\n",
    "train_None = pd.read_csv('None_order_train_.csv', dtype={\n",
    "            'order_id': np.uint32,\n",
    "            'product_id': np.uint16,\n",
    "            'add_to_cart_order': np.uint16,\n",
    "            'reordered': np.uint16})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior = pd.concat([prior, prior_None], ignore_index = True)\n",
    "train = pd.concat([train, train_None], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# order.set_index('order_id', inplace=True, drop=False)\n",
    "# reorder_info = pd.DataFrame()\n",
    "# reorder_info['order_id'] = prior.groupby(prior.order_id)['order_id'].apply(lambda x: x.iloc[0])\n",
    "# reorder_info['user_id'] = reorder_info.order_id.map(order.user_id)\n",
    "# reorder_info['nb_reorder'] = prior.groupby(prior.order_id)['reordered'].sum().astype(np.uint8)\n",
    "# reorder_info['last_add_to_cart_order'] = prior.groupby(prior.order_id)['add_to_cart_order'].max()\n",
    "# reorder_info['order_number'] = reorder_info.order_id.map(order.order_number)\n",
    "# none_order_id = np.array(reorder_info.order_id[reorder_info.nb_reorder == 0])\n",
    "# d = dict()\n",
    "# d['order_id'] = []\n",
    "# d['product_id'] = []\n",
    "# d['add_to_cart_order'] = []\n",
    "# d['reordered'] = []\n",
    "# count=0\n",
    "# for k in none_order_id:\n",
    "#     count+=1\n",
    "#     if not count % 10000:\n",
    "#         print(\"%d orders\" % count)\n",
    "#     order_number = np.array(order.order_number[order.order_id == k])[0]\n",
    "#     if order_number > 1: # 第一个order不要\n",
    "#         d['order_id'].append(k)\n",
    "#         d['product_id'].append(49689)\n",
    "#         d['add_to_cart_order'].append(np.array(reorder_info.last_add_to_cart_order[reorder_info.order_id==k])[0] + 1)\n",
    "#         if order_number == 2: # 第二个order不算reorder\n",
    "#             d['reordered'].append(0)\n",
    "#         else:\n",
    "#             user_id = np.array(reorder_info.user_id[reorder_info.order_id==k])[0]\n",
    "#             user_orders = order[order.user_id==user_id]\n",
    "#             last_order_id = np.array(user_orders.order_id[user_orders.order_number == (order_number - 1)])[0]\n",
    "#             if np.array(reorder_info.nb_reorder[reorder_info.order_id==last_order_id])[0] == 0:\n",
    "#                 d['reordered'].append(1)\n",
    "#             else:\n",
    "#                 d['reordered'].append(0)\n",
    "# tmp = pd.DataFrame()\n",
    "# tmp['order_id'] = d['order_id']\n",
    "# tmp['product_id'] = d['product_id']\n",
    "# tmp['add_to_cart_order'] = d['add_to_cart_order']\n",
    "# tmp['reordered'] = d['reordered']\n",
    "# tmp.to_csv('None_order_prior.csv', index=False)\n",
    "# reorder_info_train = pd.DataFrame()\n",
    "# reorder_info_train['order_id'] = train.groupby(train.order_id)['order_id'].apply(lambda x: x.iloc[0])\n",
    "# reorder_info_train['user_id'] = reorder_info_train.order_id.map(order.user_id)\n",
    "# reorder_info_train['nb_reorder'] = train.groupby(train.order_id)['reordered'].sum().astype(np.uint8)\n",
    "# reorder_info_train['last_add_to_cart_order'] = train.groupby(train.order_id)['add_to_cart_order'].max()\n",
    "# reorder_info_train['order_number'] = reorder_info_train.order_id.map(order.order_number)\n",
    "# none_order_id_train = np.array(reorder_info_train.order_id[reorder_info_train.nb_reorder == 0])\n",
    "\n",
    "# d = dict()\n",
    "# d['order_id'] = []\n",
    "# d['product_id'] = []\n",
    "# d['add_to_cart_order'] = []\n",
    "# d['reordered'] = []\n",
    "# count=0\n",
    "# for k in none_order_id_train:\n",
    "#     count+=1\n",
    "#     if not count % 10000:\n",
    "#         print(\"%d orders\" % count)\n",
    "#     order_number = np.array(order.order_number[order.order_id == k])[0]\n",
    "#     d['order_id'].append(k)\n",
    "#     d['product_id'].append(49689)\n",
    "#     d['add_to_cart_order'].append(np.array(reorder_info_train.last_add_to_cart_order[reorder_info_train.order_id==k])[0] + 1)\n",
    "#     user_id = np.array(reorder_info_train.user_id[reorder_info_train.order_id==k])[0]\n",
    "#     user_orders = order[order.user_id==user_id]\n",
    "# #     display(user_orders)\n",
    "#     last_order_id = np.array(user_orders.order_id[user_orders.order_number == (order_number - 1)])[0]\n",
    "#     if np.array(reorder_info.nb_reorder[reorder_info.order_id==last_order_id])[0] == 0:\n",
    "#         d['reordered'].append(1)\n",
    "#     else:\n",
    "#         d['reordered'].append(0)\n",
    "# tmp = pd.DataFrame()\n",
    "# tmp['order_id'] = d['order_id']\n",
    "# tmp['product_id'] = d['product_id']\n",
    "# tmp['add_to_cart_order'] = d['add_to_cart_order']\n",
    "# tmp['reordered'] = d['reordered']\n",
    "# tmp.to_csv('None_order_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best sellers\n",
    "best_seller_id = [24852, 13176, 21137, 21903, 47626, 47766, 47209, 16797, \\\n",
    "                 26209, 27966]\n",
    "most_often_reordered = [1729, 20940, 12193, 21038, 31764, 24852, 117, \\\n",
    "                       39180, 12384, 24024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_orders = order[order.eval_set == 'train']\n",
    "test_orders = order[order.eval_set == 'test']\n",
    "prior_orders = order[order.eval_set == 'prior']\n",
    "\n",
    "train.set_index(['order_id', 'product_id'], inplace=True, drop=False)\n",
    "\n",
    "order.set_index('order_id', inplace=True, drop=False)\n",
    "prior = prior.join(order, on='order_id', rsuffix='_')\n",
    "prior.drop('order_id_', inplace=True, axis=1)\n",
    "prior.set_index('order_id', inplace=True, drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Construct products information...\n"
     ]
    }
   ],
   "source": [
    "print('Construct products information...')\n",
    "prods = pd.DataFrame()\n",
    "prods['total_nb'] = prior.groupby(prior.product_id).size().astype(np.uint32)\n",
    "prods['nb_reorder'] = prior.groupby(prior.product_id)['reordered'].sum().astype(np.uint32)\n",
    "prods['reorder_rate'] = prods.nb_reorder / prods.total_nb.astype(np.float32)\n",
    "prods['nb_buyers'] = prior.groupby(prior.product_id)['user_id'].apply(lambda x: len(set(x))).astype(np.uint16) # unique buyers\n",
    "prods['avg_add_to_cart_order'] = prior.groupby(prior.product_id)['add_to_cart_order'].mean().astype(np.uint8)\n",
    "prods['min_add_to_cart_order'] = prior.groupby(prior.product_id)['add_to_cart_order'].min().astype(np.uint8)\n",
    "prods['max_add_to_cart_order'] = prior.groupby(prior.product_id)['add_to_cart_order'].max().astype(np.uint8)\n",
    "prods['nb_orders'] = prior.groupby(prior.product_id).size().astype(np.uint16)\n",
    "products = products.join(prods, on='product_id')\n",
    "products.set_index('product_id', drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order related information...\n"
     ]
    }
   ],
   "source": [
    "print('Order related information...')\n",
    "ords_pri = pd.DataFrame()\n",
    "ords_pri['order_id'] = prior.groupby(prior.order_id)['order_id'].apply(lambda x: x.iloc[0])\n",
    "ords_pri['user_id'] = prior.groupby(prior.order_id)['user_id'].apply(lambda x: x.iloc[0])\n",
    "ords_pri['unique_product_id'] = prior.groupby(prior.order_id)['product_id'].apply(set)\n",
    "ords_pri['all_product_id'] = prior.groupby(prior.order_id)['product_id'].apply(list)\n",
    "ords_pri.set_index('order_id', drop=False, inplace=True)\n",
    "ords_pri['nb_items'] = prior.groupby(prior.order_id)['product_id'].size().astype(np.uint8)\n",
    "ords_pri['first_item_id'] = prior.groupby(prior.order_id)['product_id'].apply(lambda x: x.iloc[0])\n",
    "ords_pri['first_item_reorder'] = prior.groupby(prior.order_id)['reordered'].apply(lambda x: x.iloc[0])\n",
    "ords_pri['nb_reorder'] = prior.groupby(prior.order_id)['reordered'].sum()\n",
    "ords_pri['reorder_ratio'] = (ords_pri['nb_reorder'] / ords_pri['nb_items']).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min and max basket size for each user...\n",
      "10000 orders...\n",
      "20000 orders...\n",
      "30000 orders...\n",
      "40000 orders...\n",
      "50000 orders...\n",
      "60000 orders...\n",
      "70000 orders...\n",
      "80000 orders...\n",
      "90000 orders...\n",
      "100000 orders...\n",
      "110000 orders...\n",
      "120000 orders...\n",
      "130000 orders...\n",
      "140000 orders...\n",
      "150000 orders...\n",
      "160000 orders...\n",
      "170000 orders...\n",
      "180000 orders...\n",
      "190000 orders...\n",
      "200000 orders...\n"
     ]
    }
   ],
   "source": [
    "print(\"min and max basket size for each user...\")\n",
    "min_basket_list = []\n",
    "max_basket_list = []\n",
    "avg_basket_list = []\n",
    "nb_reorder_items_list = []\n",
    "avg_reorder_per_basket_list = []\n",
    "reorder_order_vs_order_ratio_list = []\n",
    "nb_all_items_list = []\n",
    "nb_reorder_items_vs_nb_all_items_ratio_list = []\n",
    "min_reorder_items_list = []\n",
    "max_reorder_items_list = []\n",
    "unique_items_set = []\n",
    "all_product_list = []\n",
    "nb_unique_items_list = []\n",
    "user_order_list = prior_orders.groupby('user_id')['order_id'].apply(list)\n",
    "for ord_i, order_list in enumerate(user_order_list):\n",
    "    if not (ord_i + 1) % 10000:\n",
    "        print(\"%d orders...\" % (ord_i+1))\n",
    "    min_basket=999\n",
    "    max_basket=0\n",
    "    min_reorder_items = 999\n",
    "    max_reorder_items = 0\n",
    "    nb_reorder = 0\n",
    "    nb_reorder_orders = 0\n",
    "    nb_all_items = 0\n",
    "    nb_order = len(order_list)\n",
    "    all_product_list_tmp = []\n",
    "    unique_items_tmp_set = set()\n",
    "    for order_id in order_list:\n",
    "        nb_item_s = ords_pri.loc[order_id, 'nb_items']\n",
    "        nb_all_items += nb_item_s\n",
    "        nb_reorder_s = ords_pri.loc[order_id, 'nb_reorder']\n",
    "        nb_reorder += nb_reorder_s\n",
    "        nb_reorder_orders += (ords_pri.loc[order_id, 'nb_reorder'] > 0).astype(np.uint8)\n",
    "        min_basket = min(min_basket, nb_item_s)\n",
    "        max_basket = max(max_basket, nb_item_s)\n",
    "        min_reorder_items = min(min_reorder_items, nb_reorder_s)\n",
    "        max_reorder_items = max(max_reorder_items, nb_reorder_s)\n",
    "        unique_items_tmp_set |= ords_pri.loc[order_id, 'unique_product_id']\n",
    "        all_product_list_tmp += ords_pri.loc[order_id, 'all_product_id']\n",
    "    unique_items_set.append(list(unique_items_tmp_set))\n",
    "    all_product_list.append(all_product_list_tmp)\n",
    "    nb_unique_items_list.append(len(unique_items_set))\n",
    "    avg_basket_list.append((nb_all_items / nb_order).astype(np.float32))\n",
    "    min_basket_list.append(min_basket)\n",
    "    max_basket_list.append(max_basket)\n",
    "    nb_reorder_items_list.append(nb_reorder)\n",
    "    avg_reorder_per_basket_list.append((nb_reorder / nb_order).astype(np.float32))\n",
    "    reorder_order_vs_order_ratio_list.append((nb_reorder_orders / nb_order).astype(np.float32)) \n",
    "    nb_all_items_list.append(nb_all_items)\n",
    "    nb_reorder_items_vs_nb_all_items_ratio_list.append((nb_reorder / nb_all_items).astype(np.float32))\n",
    "    min_reorder_items_list.append(min_reorder_items)\n",
    "    max_reorder_items_list.append(max_reorder_items)\n",
    "#     print(\"order id: %d, min_basket: %d, max_basket: %d\" %(order_id, min_basket, max_basket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_orders.set_index('user_id', drop=False, inplace=True)\n",
    "prior_orders.set_index('user_id', drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User related information\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:20: FutureWarning: 'user_id' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "/opt/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:21: FutureWarning: 'user_id' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "/opt/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:22: FutureWarning: 'user_id' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "/opt/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:23: FutureWarning: 'user_id' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n",
      "/opt/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:24: FutureWarning: 'user_id' is both a column name and an index level.\n",
      "Defaulting to column but this will raise an ambiguity error in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "users best seller 1\n",
      "users best reorder 1\n"
     ]
    }
   ],
   "source": [
    "print('User related information')\n",
    "users = pd.DataFrame()\n",
    "users['user_id'] = prior.groupby('user_id')['user_id'].apply(lambda x: x.iloc[0])\n",
    "users['nb_order'] = prior_orders.groupby(users.user_id).size().astype(np.uint16)\n",
    "users['orders'] = prior_orders.groupby(users.user_id)['order_id'].apply(list) \n",
    "users['min_basket'] = min_basket_list\n",
    "users['max_basket'] = max_basket_list\n",
    "users['nb_reorder_items'] = nb_reorder_items_list\n",
    "users['avg_reorder_per_basket'] = avg_reorder_per_basket_list\n",
    "users['reorder_order_vs_order_ratio'] = reorder_order_vs_order_ratio_list\n",
    "users['nb_all_items'] = nb_all_items_list\n",
    "users['nb_reorder_items_vs_nb_all_items_ratio'] = nb_reorder_items_vs_nb_all_items_ratio_list\n",
    "users['min_reorder_items'] = min_reorder_items_list\n",
    "users['max_reorder_items'] = max_reorder_items_list\n",
    "users['nb_unique_items'] = nb_unique_items_list\n",
    "users['avg_basket'] = avg_basket_list\n",
    "users['all_products'] = all_product_list\n",
    "users['unique_products'] = unique_items_set\n",
    "#     print(\"order id: %d, min_basket: %d, max_basket: %d\" %(order_id, min_basket, max_basket))\n",
    "users['avg_days_between_order'] = prior_orders.groupby('user_id')['days_since_prior_order'].mean().astype(np.float32)\n",
    "users['sum_days_between_order'] = prior_orders.groupby('user_id')['days_since_prior_order'].sum().astype(np.uint16)\n",
    "users['avg_hour_of_day'] = prior_orders.groupby('user_id')['order_hour_of_day'].mean().astype(np.float32)\n",
    "users['min_days_of_week'] = prior_orders.groupby('user_id')['order_dow'].apply(min).astype(np.uint8)\n",
    "users['max_days_of_week'] = prior_orders.groupby('user_id')['order_dow'].apply(max).astype(np.uint8)\n",
    "print('users best seller 1')\n",
    "users['nb_1st_seller'] = users.all_products.apply(lambda X: sum([best_seller_id[0]==x for x in X]))\n",
    "print('users best reorder 1')\n",
    "users['nb_1st_reorder'] = users.all_products.apply(lambda X: sum([most_often_reordered[0] == x for x in X]))\n",
    "\n",
    "# Query data from ords\n",
    "users['last_order_id'] = prior_orders.groupby(prior_orders.user_id)['order_id'].apply(lambda x: x.iloc[-1])\n",
    "users['lo_nb_products'] = users.last_order_id.map(ords_pri.nb_items)\n",
    "users['lo_first_item_id'] = users.last_order_id.map(ords_pri.first_item_id)\n",
    "users['lo_first_item_reorder'] = users.last_order_id.map(ords_pri.first_item_reorder)\n",
    "users['lo_nb_reorder'] = users.last_order_id.map(ords_pri.nb_reorder)\n",
    "users['lo_reorder_ratio'] = users.last_order_id.map(ords_pri.reorder_ratio)\n",
    "\n",
    "users['last_2_order_id'] = prior_orders.groupby(prior_orders.user_id)['order_id'].apply(lambda x: x.iloc[-2])\n",
    "users['lo2_nb_products'] = users.last_2_order_id.map(ords_pri.nb_items)\n",
    "users['lo2_first_item_id'] = users.last_2_order_id.map(ords_pri.first_item_id)\n",
    "users['lo2_first_item_reorder'] = users.last_2_order_id.map(ords_pri.first_item_reorder)\n",
    "users['lo2_nb_reorder'] = users.last_2_order_id.map(ords_pri.nb_reorder)\n",
    "users['lo2_reorder_ratio'] = users.last_2_order_id.map(ords_pri.reorder_ratio)\n",
    "\n",
    "users['last_3_order_id'] = prior_orders.groupby(prior_orders.user_id)['order_id'].apply(lambda x: x.iloc[-3])\n",
    "users['lo3_nb_products'] = users.last_3_order_id.map(ords_pri.nb_items)\n",
    "users['lo3_first_item_id'] = users.last_3_order_id.map(ords_pri.first_item_id)\n",
    "users['lo3_first_item_reorder'] = users.last_3_order_id.map(ords_pri.first_item_reorder)\n",
    "users['lo3_nb_reorder'] = users.last_3_order_id.map(ords_pri.nb_reorder)\n",
    "users['lo3_reorder_ratio'] = users.last_3_order_id.map(ords_pri.reorder_ratio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UserXproduct_id information...\n"
     ]
    }
   ],
   "source": [
    "print('UserXproduct_id information...')\n",
    "prior['user_product_index'] = (prior.user_id.astype(np.uint64) * 100000\\\n",
    "                               + prior.product_id).astype(np.uint64)\n",
    "d = dict()\n",
    "for row in prior.itertuples():\n",
    "    k = row.user_product_index\n",
    "    if k not in d:\n",
    "        d[k] = (1, \\\n",
    "                row.add_to_cart_order, \\\n",
    "                row.reordered, \\\n",
    "                (row.order_number, row.order_id),\\\n",
    "                row.order_dow, \\\n",
    "                row.order_hour_of_day, \\\n",
    "                row.add_to_cart_order, \\\n",
    "                row.add_to_cart_order, \\\n",
    "                (row.order_number == 1).astype(np.uint8), \\\n",
    "                (row.order_number == 2).astype(np.uint8), \\\n",
    "                (row.order_number, row.order_id))\n",
    "    else:\n",
    "        d[k] = (d[k][0]+1, d[k][1]+row.add_to_cart_order, \\\n",
    "                d[k][2]+row.reordered, \\\n",
    "                # find last order with that product\n",
    "                max(d[k][3], (row.order_number, row.order_id)), \\\n",
    "                d[k][4]+row.order_dow, \\\n",
    "                d[k][5]+row.order_hour_of_day, \\\n",
    "                min(d[k][6], row.add_to_cart_order), \\\n",
    "                max(d[k][7], row.add_to_cart_order), \\\n",
    "                d[k][8]+(row.order_number == 1).astype(np.uint8), \\\n",
    "                d[k][9]+(row.order_number == 2).astype(np.uint8), \\\n",
    "                min(d[k][10], (row.order_number, row.order_id)))\n",
    "UserProduct = pd.DataFrame.from_dict(d, orient='index')\n",
    "del d\n",
    "UserProduct.columns = ['nb_orders', 'sum_add_to_cart_order', 'nb_reordered', \\\n",
    "                      'last_order_id', 'sum_order_dow', 'sum_order_hour_of_day', \\\n",
    "                      'min_add_to_cart_order', 'max_add_to_cart_order', \\\n",
    "                       'buy_first_time_total_nb', 'buy_second_time_total_nb', \\\n",
    "                       'first_order_id']\n",
    "UserProduct['nb_orders'] = UserProduct.nb_orders.astype(np.uint16) \n",
    "UserProduct['sum_add_to_cart_order'] = UserProduct.sum_add_to_cart_order.astype(np.uint16)\n",
    "UserProduct['nb_reordered'] = UserProduct.nb_reordered.astype(np.uint16)\n",
    "UserProduct['last_order_id'] = UserProduct.last_order_id.map(lambda x: x[1]).astype(np.uint32)\n",
    "UserProduct['last_order_number'] = UserProduct.last_order_id.map(order.order_number)\n",
    "UserProduct['first_order_id'] = UserProduct.first_order_id.map(lambda x: x[1]).astype(np.uint32)\n",
    "UserProduct['first_order_number'] = UserProduct.first_order_id.map(order.order_number)\n",
    "UserProduct['sum_order_dow'] = UserProduct.sum_order_dow.astype(np.uint16)\n",
    "UserProduct['sum_order_hour_of_day'] = UserProduct.sum_order_hour_of_day.astype(np.uint32)\n",
    "UserProduct['min_add_to_cart_order'] = UserProduct.min_add_to_cart_order.astype(np.uint8)\n",
    "UserProduct['max_add_to_cart_order'] = UserProduct.max_add_to_cart_order.astype(np.uint8)\n",
    "UserProduct['buy_first_time_total_nb'] = UserProduct.buy_first_time_total_nb\n",
    "UserProduct['buy_second_time_total_nb'] = UserProduct.buy_second_time_total_nb\n",
    "UserProduct['reorder_second_time_VS_first_time'] = (UserProduct.buy_second_time_total_nb - \\\n",
    "                                                    UserProduct.buy_first_time_total_nb).astype(np.float32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_features(orders, labels_out=False):\n",
    "    print('generate features and labels(optional) from selected orders')\n",
    "    count=0\n",
    "    product_list = []\n",
    "    order_list = []\n",
    "    labels = []\n",
    "    for row in orders.itertuples():\n",
    "        count+=1\n",
    "        order_id = row.order_id\n",
    "        user_id = row.user_id\n",
    "        user_products = users.unique_products[user_id]\n",
    "        product_list += user_products\n",
    "        order_list += [order_id] * len(user_products)\n",
    "        if labels_out:\n",
    "            labels += [(order_id, product) in train.index for product in user_products]            \n",
    "        if count%10000 == 0:\n",
    "            print('order row', count)\n",
    "            \n",
    "    df = pd.DataFrame({'order_id': order_list, 'product_id': product_list}, dtype=np.int32)\n",
    "    labels = np.array(labels, dtype=np.int8)\n",
    "    del order_list\n",
    "    del product_list\n",
    "    \n",
    "    print(\"user related features<prior>\")\n",
    "    df['user_id'] = df.order_id.map(order.user_id)\n",
    "    df['user_total_orders'] = df.user_id.map(users.nb_order)\n",
    "    df['user_avg_days_between_orders'] = df.user_id.map(users.avg_days_between_order)\n",
    "    df['user_sum_days_between_orders'] = df.user_id.map(users.sum_days_between_order)\n",
    "    df['user_min_days_of_week'] = df.user_id.map(users.min_days_of_week)\n",
    "    df['user_max_days_of_week'] = df.user_id.map(users.max_days_of_week)\n",
    "    df['user_avg_hour_of_day'] = df.user_id.map(users.avg_hour_of_day)\n",
    "    \n",
    "    df['user_nb_1st_seller'] = df.user_id.map(users.nb_1st_seller)\n",
    "\n",
    "    df['user_nb_1st_reorder'] = df.user_id.map(users.nb_1st_reorder)\n",
    "    \n",
    "    df['user_last_order_id'] = df.user_id.map(users.last_order_id)\n",
    "    df['user_lo_dow'] = df.user_last_order_id.map(order.order_dow)\n",
    "    df['user_lo_hour_of_day'] = df.user_last_order_id.map(order.order_hour_of_day)\n",
    "    df['user_lo_day_since_prior'] = df.user_last_order_id.map(order.days_since_prior_order)\n",
    "    df['user_lo_nb_products'] = df.user_id.map(users.lo_nb_products)\n",
    "    df['user_lo_first_item_id'] = df.user_id.map(users.lo_first_item_id)\n",
    "    df['user_lo_first_item_reorder'] = df.user_id.map(users.lo_first_item_reorder)\n",
    "    df['user_lo_nb_reorder'] = df.user_id.map(users.lo_nb_reorder)\n",
    "    df['user_lo_reorder_ratio'] = df.user_id.map(users.lo_reorder_ratio)\n",
    "    \n",
    "    df['user_last2_order_id'] = df.user_id.map(users.last_2_order_id)\n",
    "    df['user_lo2_dow'] = df.user_last2_order_id.map(order.order_dow)\n",
    "    df['user_lo2_hour_of_day'] = df.user_last2_order_id.map(order.order_hour_of_day)\n",
    "    df['user_lo2_day_since_prior'] = df.user_last2_order_id.map(order.days_since_prior_order)\n",
    "    df['user_lo2_nb_products'] = df.user_id.map(users.lo2_nb_products)\n",
    "    df['user_lo2_first_item_id'] = df.user_id.map(users.lo2_first_item_id)\n",
    "    df['user_lo2_first_item_reorder'] = df.user_id.map(users.lo2_first_item_reorder)\n",
    "    df['user_lo2_nb_reorder'] = df.user_id.map(users.lo2_nb_reorder)\n",
    "    df['user_lo2_reorder_ratio'] = df.user_id.map(users.lo2_reorder_ratio)\n",
    "    \n",
    "    df['user_last3_order_id'] = df.user_id.map(users.last_3_order_id)\n",
    "    df['user_lo3_dow'] = df.user_last3_order_id.map(order.order_dow)\n",
    "    df['user_lo3_hour_of_day'] = df.user_last3_order_id.map(order.order_hour_of_day)\n",
    "    df['user_lo3_day_since_prior'] = df.user_last3_order_id.map(order.days_since_prior_order)\n",
    "    df['user_lo3_nb_products'] = df.user_id.map(users.lo3_nb_products)\n",
    "    df['user_lo3_first_item_id'] = df.user_id.map(users.lo3_first_item_id)\n",
    "    df['user_lo3_first_item_reorder'] = df.user_id.map(users.lo3_first_item_reorder)\n",
    "    df['user_lo3_nb_reorder'] = df.user_id.map(users.lo3_nb_reorder)\n",
    "    df['user_lo3_reorder_ratio'] = df.user_id.map(users.lo3_reorder_ratio)\n",
    "    \n",
    "    df['users_min_basket'] = df.user_id.map(users.min_basket)\n",
    "    df['users_max_basket'] = df.user_id.map(users.max_basket)\n",
    "    df['users_avg_basket'] = df.user_id.map(users.avg_basket)\n",
    "    df['users_nb_unique_items'] = df.user_id.map(users.nb_unique_items)\n",
    "    df['users_nb_reorder_items'] = df.user_id.map(users.nb_reorder_items)\n",
    "    df['users_avg_reorder_per_basket'] = df.user_id.map(users.avg_reorder_per_basket)\n",
    "    df['users_reorder_order_vs_order_ratio'] = df.user_id.map(users.reorder_order_vs_order_ratio)\n",
    "    df['users_nb_all_items'] = df.user_id.map(users.nb_all_items)\n",
    "    df['users_nb_reorder_items_vs_nb_all_items_ratio'] = df.user_id.map(users.nb_reorder_items_vs_nb_all_items_ratio)\n",
    "    df['users_min_reorder_items'] = df.user_id.map(users.min_reorder_items)\n",
    "    df['users_max_reorder_items'] = df.user_id.map(users.max_reorder_items)\n",
    "    \n",
    "    print(\"product related features<prior>\")\n",
    "    df['product_aisle_id'] = df.product_id.map(products.aisle_id)\n",
    "    df['product_department_id'] = df.product_id.map(products.department_id)\n",
    "    df['product_orders'] = df.product_id.map(products.total_nb)\n",
    "    df['product_reorders'] = df.product_id.map(products.nb_reorder)\n",
    "    df['product_reorder_rate'] = df.product_id.map(products.reorder_rate)\n",
    "    df['product_nb_buyers'] = df.product_id.map(products.nb_buyers)\n",
    "    df['product_avg_add_to_cart_order'] = df.product_id.map(products.avg_add_to_cart_order)\n",
    "    df['product_nb_orders'] = df.product_id.map(products.nb_orders)\n",
    "    df['product_min_add_to_cart_order'] = df.product_id.map(products.min_add_to_cart_order)\n",
    "    df['product_max_add_to_cart_order'] = df.product_id.map(products.max_add_to_cart_order)\n",
    "    \n",
    "    print(\"order related features<train>\")\n",
    "    df['order_hour_of_day'] = df.order_id.map(order.order_hour_of_day)\n",
    "    df['order_days_since_prior_order'] = df.order_id.map(order.days_since_prior_order)\n",
    "    df['order_day_of_week'] = df.order_id.map(order.order_dow)\n",
    "    df['order_number'] = df.order_id.map(order.order_number)\n",
    "    \n",
    "    print(\"userXproduct related features<prior>\")\n",
    "    # 1.nb_orders, 2.sum_add_to_cart_order, 3.nb_reordered, \\\n",
    "    # 4.last_order_id, 5.sum_order_dow, 6.sum_order_hour_of_day\n",
    "    df['UP'] = df.product_id+df.user_id.astype(np.uint64)*100000\n",
    "    df['UP_nb_orders'] = df.UP.map(UserProduct.nb_orders)\n",
    "    df['UP_avg_add_to_cart_order'] = df.UP.map(UserProduct.sum_add_to_cart_order)\\\n",
    "                                    / df.UP_nb_orders\n",
    "    df['UP_nb_reordered'] = df.UP.map(UserProduct.nb_reordered)\n",
    "    df['UP_reorder_ratio'] = (df.UP_nb_reordered / df.UP_nb_orders).astype(np.float32)\n",
    "    df['UP_last_order_id'] = df.UP.map(UserProduct.last_order_id)\n",
    "    df['UP_last_order_number'] = df.UP.map(UserProduct.last_order_number)\n",
    "    df['UP_first_order_id'] = df.UP.map(UserProduct.first_order_id)\n",
    "    df['UP_first_order_number'] = df.UP.map(UserProduct.first_order_number)\n",
    "    df['UP_avg_order_dow'] = (df.UP.map(UserProduct.sum_order_dow)\\\n",
    "                              / df.UP_nb_orders).astype(np.float32)\n",
    "    df['UP_avg_order_hour_of_day'] = (df.UP.map(UserProduct.sum_order_hour_of_day) / \\\n",
    "                                    df.UP_nb_orders).astype(np.float32)\n",
    "    df['UP_order_ratio'] = (df.UP_nb_orders / df.user_total_orders).astype(np.float32)\n",
    "    df['UP_order_since_last'] = df.user_total_orders - \\\n",
    "                                df.UP_last_order_id.map(order.order_number)\n",
    "    #最后一次买该产品和该订单-相同产品相隔的时间(没有算日期。。。)\n",
    "    df['UP_delta_hour_vs_last'] = abs(df.order_hour_of_day - df.UP_last_order_id.map(\\\n",
    "                                       order.order_hour_of_day)).map(lambda x: min(x, 24-x)).astype(np.int8)\n",
    "    df['UP_min_add_to_cart_order'] = df.UP.map(UserProduct.min_add_to_cart_order)\n",
    "    df['UP_max_add_to_cart_order'] = df.UP.map(UserProduct.max_add_to_cart_order)\n",
    "    df['UP_buy_first_time_total_nb'] = df.UP.map(UserProduct.buy_first_time_total_nb)\n",
    "    df['UP_buy_second_time_total_nb'] = df.UP.map(UserProduct.buy_second_time_total_nb)\n",
    "    df['UP_reorder_second_time_VS_first_time'] = df.UP.map(UserProduct.reorder_second_time_VS_first_time)\n",
    "    df['UP_nb_orders_since_last_order'] = df['user_total_orders'] - df['UP_last_order_number']\n",
    "    df.drop('UP', axis=1, inplace=True)\n",
    "    \n",
    "#     print(df.dtypes)\n",
    "#     print(df.memory_usage())\n",
    "    return(df, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('train order size: ', train_orders.shape)\n",
    "df_train, labels = gen_features(train_orders, labels_out=True)\n",
    "print('feature size: ', df_train.shape)\n",
    "df_train['label'] = pd.Series(labels, dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test order size:  (75000, 7)\n",
      "generate features and labels(optional) from selected orders\n",
      "order row 10000\n",
      "order row 20000\n",
      "order row 30000\n",
      "order row 40000\n",
      "order row 50000\n",
      "order row 60000\n",
      "order row 70000\n",
      "user related features<prior>\n",
      "product related features<prior>\n",
      "order related features<train>\n",
      "userXproduct related features<prior>\n"
     ]
    }
   ],
   "source": [
    "print('test order size: ', test_orders.shape)\n",
    "df_test, _ = gen_features(test_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train['label'] = pd.Series(labels, dtype=np.int8)\n",
    "user_id_list=users.index.tolist()\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(user_id_list)\n",
    "nb_user = len(user_id_list)\n",
    "val_nb_user = nb_user // 15\n",
    "# del users\n",
    "# del order\n",
    "# del UserProduct\n",
    "# del products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"user size: \", nb_user)\n",
    "for nb in range(15):\n",
    "    print(\"Doing Cross %d Validation\"% nb)\n",
    "    val_start = nb*val_nb_user\n",
    "    val_end = (nb+1)*val_nb_user\n",
    "    if nb==14:\n",
    "        val_end = nb_user\n",
    "    print(\"validation start: %d, end: %d\" % (val_start, val_end))\n",
    "    train_user_ids = np.concatenate((user_id_list[:val_start],\\\n",
    "                                    user_id_list[val_end:]))\n",
    "    val_user_ids = user_id_list[val_start:val_end]\n",
    "    print(\"Split train and valid data/label by user_id\")\n",
    "    sub_df_val = df_train[df_train.user_id.isin(val_user_ids)].copy()\n",
    "    sub_df_train = df_train[df_train.user_id.isin(train_user_ids)].copy()\n",
    "    sub_train_label = np.array(sub_df_train['label'])\n",
    "    sub_val_label = np.array(sub_df_val['label'])\n",
    "    sub_df_train.drop(['label'], axis=1, inplace=True)\n",
    "    sub_df_val.drop(['label'], axis=1, inplace=True)\n",
    "    params={\n",
    "    'objective': 'reg:logistic', \n",
    "    'eval_metric': 'logloss',\n",
    "    'gamma':0.7,  # 用于控制是否后剪枝的参数,越大越保守，一般0.1、0.2这样子。\n",
    "    'max_depth':10, # 构建树的深度，越大越容易过拟合\n",
    "    'lambda':10,  # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。\n",
    "    'subsample':0.76, # 随机采样训练样本\n",
    "    'colsample_bytree':0.95, # 生成树时进行的列采样\n",
    "    'min_child_weight':10,  \n",
    "    'silent':0 ,#设置成1则没有运行信息输出，最好是设置为0.\n",
    "    'eta': 0.1, # 如同学习率\n",
    "    'nthread':8,# cpu 线程数\n",
    "    }\n",
    "    train = np.array(sub_df_train)\n",
    "    valid = np.array(sub_df_val)\n",
    "    n = 100\n",
    "    plst = list(params.items())\n",
    "    xgtrain = xgb.DMatrix(train, label=sub_train_label)\n",
    "    xgval = xgb.DMatrix(valid, label=sub_val_label)\n",
    "    watchlist = [(xgtrain, 'train'), (xgval, 'val')]\n",
    "    model = xgb.train(plst, xgtrain, n, watchlist, early_stopping_rounds=100)\n",
    "    model.save_model('CV_0807_'+str(nb)+'.model')\n",
    "    del sub_df_train\n",
    "    del sub_df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost predict from model 0\n"
     ]
    }
   ],
   "source": [
    "df_test_array = np.array(df_test)\n",
    "xgtest = xgb.DMatrix(df_test_array)\n",
    "xgbst = xgb.Booster()\n",
    "for i in range(1):\n",
    "    print('xgboost predict from model',str(i))\n",
    "    xgbst.load_model('./CV_0807_'+str(i)+'.model')\n",
    "    if i == 0:\n",
    "        preds = xgbst.predict(xgtest)\n",
    "    else:\n",
    "        preds += xgbst.predict(xgtest)\n",
    "# preds = preds / 15.\n",
    "df_test['preds']=preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "THRESHOLD=0.2\n",
    "d = dict()\n",
    "for row in df_test.itertuples():\n",
    "    if row.preds > THRESHOLD:\n",
    "        try:\n",
    "            d[row.order_id] += ' ' + str(row.product_id)\n",
    "        except:\n",
    "            d[row.order_id] = str(row.product_id)\n",
    "for order in test_orders.order_id:\n",
    "    if order not in d:\n",
    "        d[order] = 'None'\n",
    "\n",
    "tst = pd.DataFrame.from_dict(d, orient='index')\n",
    "tst.reset_index(inplace=True)\n",
    "tst.columns = ['order_id', 'products']\n",
    "tst.to_csv('submission_82_features_THRESHOLD_0.2_15_fold_CV_0809.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "@author: Faron\n",
    "\"\"\"\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pylab as plt\n",
    "from datetime import datetime\n",
    "\n",
    "'''\n",
    "This kernel implements the O(n²) F1-Score expectation maximization algorithm presented in\n",
    "\"Ye, N., Chai, K., Lee, W., and Chieu, H.  Optimizing F-measures: A Tale of Two Approaches. In ICML, 2012.\"\n",
    "\n",
    "It solves argmax_(0 <= k <= n,[[None]]) E[F1(P,k,[[None]])]\n",
    "with [[None]] being the indicator for predicting label \"None\"\n",
    "given posteriors P = [p_1, p_2, ... , p_n], where p_1 > p_2 > ... > p_n\n",
    "under label independence assumption by means of dynamic programming in O(n²).\n",
    "'''\n",
    "\n",
    "\n",
    "class F1Optimizer():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def get_expectations(P, pNone=None):\n",
    "        expectations = []\n",
    "        P = np.sort(P)[::-1]\n",
    "\n",
    "        n = np.array(P).shape[0]\n",
    "        DP_C = np.zeros((n + 2, n + 1))\n",
    "        if pNone is None:\n",
    "            pNone = (1.0 - P).prod()\n",
    "\n",
    "        DP_C[0][0] = 1.0\n",
    "        for j in range(1, n):\n",
    "            DP_C[0][j] = (1.0 - P[j - 1]) * DP_C[0, j - 1]\n",
    "\n",
    "        for i in range(1, n + 1):\n",
    "            DP_C[i, i] = DP_C[i - 1, i - 1] * P[i - 1]\n",
    "            for j in range(i + 1, n + 1):\n",
    "                DP_C[i, j] = P[j - 1] * DP_C[i - 1, j - 1] + (1.0 - P[j - 1]) * DP_C[i, j - 1]\n",
    "\n",
    "        DP_S = np.zeros((2 * n + 1,))\n",
    "        DP_SNone = np.zeros((2 * n + 1,))\n",
    "        for i in range(1, 2 * n + 1):\n",
    "            DP_S[i] = 1. / (1. * i)\n",
    "            DP_SNone[i] = 1. / (1. * i + 1)\n",
    "        for k in range(n + 1)[::-1]:\n",
    "            f1 = 0\n",
    "            f1None = 0\n",
    "            for k1 in range(n + 1):\n",
    "                f1 += 2 * k1 * DP_C[k1][k] * DP_S[k + k1]\n",
    "                f1None += 2 * k1 * DP_C[k1][k] * DP_SNone[k + k1]\n",
    "            for i in range(1, 2 * k - 1):\n",
    "                DP_S[i] = (1 - P[k - 1]) * DP_S[i] + P[k - 1] * DP_S[i + 1]\n",
    "                DP_SNone[i] = (1 - P[k - 1]) * DP_SNone[i] + P[k - 1] * DP_SNone[i + 1]\n",
    "            expectations.append([f1None + 2 * pNone / (2 + k), f1])\n",
    "\n",
    "        return np.array(expectations[::-1]).T\n",
    "\n",
    "    @staticmethod\n",
    "    def maximize_expectation(P, pNone=None):\n",
    "        expectations = F1Optimizer.get_expectations(P, pNone)\n",
    "\n",
    "        ix_max = np.unravel_index(expectations.argmax(), expectations.shape)\n",
    "        max_f1 = expectations[ix_max]\n",
    "\n",
    "        predNone = True if ix_max[0] == 0 else False\n",
    "        best_k = ix_max[1]\n",
    "\n",
    "        return best_k, predNone, max_f1\n",
    "\n",
    "    @staticmethod\n",
    "    def _F1(tp, fp, fn):\n",
    "        return 2 * tp / (2 * tp + fp + fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def _Fbeta(tp, fp, fn, beta=1.0):\n",
    "        beta_squared = beta ** 2\n",
    "        return (1.0 + beta_squared) * tp / ((1.0 + beta_squared) * tp + fp + beta_squared * fn)\n",
    "\n",
    "\n",
    "def print_best_prediction(P, pNone=None):\n",
    "    print(\"Maximize F1-Expectation\")\n",
    "    print(\"=\" * 23)\n",
    "    P = np.sort(P)[::-1]\n",
    "    n = P.shape[0]\n",
    "    L = ['L{}'.format(i + 1) for i in range(n)]\n",
    "\n",
    "    if pNone is None:\n",
    "        print(\"Estimate p(None|x) as (1-p_1)*(1-p_2)*...*(1-p_n)\")\n",
    "        pNone = (1.0 - P).prod()\n",
    "\n",
    "    PL = ['p({}|x)={}'.format(l, p) for l, p in zip(L, P)]\n",
    "    print(\"Posteriors: {} (n={})\".format(PL, n))\n",
    "    print(\"p(None|x)={}\".format(pNone))\n",
    "\n",
    "    opt = F1Optimizer.maximize_expectation(P, pNone)\n",
    "    best_prediction = ['None'] if opt[1] else []\n",
    "    best_prediction += (L[:opt[0]])\n",
    "    f1_max = opt[2]\n",
    "\n",
    "    print(\"Prediction {} yields best E[F1] of {}\\n\".format(best_prediction, f1_max))\n",
    "\n",
    "\n",
    "def save_plot(P, filename='expected_f1.png'):\n",
    "    E_F1 = pd.DataFrame(F1Optimizer.get_expectations(P).T, columns=[\"/w None\", \"/wo None\"])\n",
    "    best_k, _, max_f1 = F1Optimizer.maximize_expectation(P)\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    E_F1.plot()\n",
    "    plt.title('Expected F1-Score for \\n {}'.format(\"P = [{}]\".format(\",\".join(map(str, P)))), fontsize=12)\n",
    "    plt.xlabel('k')\n",
    "    plt.xticks(np.arange(0, len(P) + 1, 1.0))\n",
    "    plt.ylabel('E[F1(P,k)]')\n",
    "    plt.plot([best_k], [max_f1], 'o', color='#000000', markersize=4)\n",
    "    plt.annotate('max E[F1(P,k)] = E[F1(P,{})] = {:.5f}'.format(best_k, max_f1), xy=(best_k, max_f1),\n",
    "                 xytext=(best_k, max_f1 * 0.8), arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=7),\n",
    "                 horizontalalignment='center', verticalalignment='top')\n",
    "    plt.gcf().savefig(filename)\n",
    "\n",
    "\n",
    "\n",
    "def timeit(P):\n",
    "    s = datetime.now()\n",
    "    F1Optimizer.maximize_expectation(P)\n",
    "    e = datetime.now()\n",
    "    return (e-s).microseconds / 1E6\n",
    "\n",
    "\n",
    "def benchmark(n=100, filename='runtimes.png'):\n",
    "    results = pd.DataFrame(index=np.arange(1,n+1))\n",
    "    results['runtimes'] = 0\n",
    "\n",
    "    for i in range(1,n+1):\n",
    "        runtimes = []\n",
    "        for j in range(5):\n",
    "            runtimes.append(timeit(np.sort(np.random.rand(i))[::-1]))\n",
    "        results.iloc[i-1] = np.mean(runtimes)\n",
    "\n",
    "    x = results.index\n",
    "    y = results.runtimes\n",
    "    results['quadratic fit'] = np.poly1d(np.polyfit(x, y, deg=2))(x)\n",
    "\n",
    "    plt.style.use('ggplot')\n",
    "    plt.figure()\n",
    "    results.plot()\n",
    "    plt.title('Expectation Maximization Runtimes', fontsize=12)\n",
    "    plt.xlabel('n = |P|')\n",
    "    plt.ylabel('time in seconds')\n",
    "    plt.gcf().savefig(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# maximize f1-score\n",
    "# 49689-->None\n",
    "# xgbst = xgb.Booster()\n",
    "# xgbst.load_model('./CV_0807_0.model')\n",
    "pred_result = pd.DataFrame()\n",
    "pred_result['order_id'] = df_test.groupby('order_id')['order_id'].apply(lambda x: x.iloc[0])\n",
    "pred_result['product_id'] = df_test.groupby('order_id')['product_id'].apply(list)\n",
    "pred_result['prob'] = df_test.groupby('order_id')['preds'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "basket_size = []\n",
    "count = 0\n",
    "for order_id in pred_result['order_id']:\n",
    "    count+=1\n",
    "    if not count % 10000:\n",
    "        print(\"count=%d, order_id=%d\" % (count, order_id))\n",
    "    product_id = np.array(pred_result.product_id[order_id])\n",
    "    prob = np.array(pred_result.prob[order_id])\n",
    "    if 49689 in product_id:\n",
    "        for i, prod_id in enumerate(product_id):\n",
    "            if prod_id==49689: # find index\n",
    "                pred_none = prob[i]\n",
    "                np.delete(prob, i, axis=0)\n",
    "                np.delete(product_id, i, axis=0)\n",
    "    else:\n",
    "        pred_none = None\n",
    "    opt = F1Optimizer.maximize_expectation(prob, pred_none)\n",
    "    if opt[1]:\n",
    "        basket_size.append(0)\n",
    "    else:\n",
    "        basket_size.append(opt[0])  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_result.to_csv('82_features_THRESHOLD_0.2_15_fold_CV_0809_pred_prob.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
